# Working Memory Graphs

Agent employs multi-head self-attention to reason over a dynamic set of vectors
representing observed and recurrent state. Significant gains in learning efficiency reported.

## Determining the state of the system

POMDP / DQN: Just condition over a concatenation of observations. Or encode them
using an LSTM.

## This work

Transformer based agend with many shortcut paths for information to
flow from past observations to current actions through memos.

### Working Memory Graphs

General idea: Apply self-attention between your "memos" and "factored observations"
coming from the system.

On each timestep, replace the oldest memo with a new memo as generated by
a nonlinear function of the core's output vector and the model parameters.

## Related Work

## Application to BabyAI

WMG-factored gets good results in terms of sample efficiency for GoToRedBall, GoToLocal, PickupLocal , need about 16,000 steps.
