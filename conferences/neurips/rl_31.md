# Orals & Spotlights Track 31: Reinforcement Learning

## Self-paced Deep Reinforcement Learning

Context: Curriculum RL

 - How to generate the curriculum?
 - Interpret curriculum as an inference problem
 - Distributions over tasks are progressively learned to approach the target task
   - Then the pace can be controlled by the agent

Task: Ball-catching from sparse rewards
 - Choose actions given the environment state.
 - Current algorithms don't generate desirable behaviour
 - one way to improve performance is to use a curriculum - create a sequence of
   tasks which guide the learning of the agent.

Contextual RL objective - agent needs to maximize performance in a variety of tasks.
 - Some tasks are easier to learn than others
 - Eg, throwing the ball towards the robot is easier to learn
 - Can we autogenerate a sequence of tasks?

Current methods:
 - intrinsic motiviation
 - success indicators

Can we create an approach that is more based on elementary quantities?

create a sequence of context distributions

 - interpolate between easy and desired tasks
 - "desiredness" of a task: $\mu(c)$ - the context distribution
 - "easiness" can be measured by the expected reward that you achieve in a given context
 - Combine these two to make a tradeoff optimization problem.
  - Basically: Pick the task distribution that gives you the highest expected reward, penalized by
    some kl-divergence between the current task disitrbution and the "ideal" task distribution.

 - Starting with a context distribution, train the RL agent, then adapt the context distribution
   based on the performance of the agent on these tasks.
 - You can use an arbitrary RL algorithm to optimize the behaviour of the agent, as long as
   you can estimate the value function.
 - Context distribution focuses more on tasks that you're interested in as you start to perform
   better in all tasks.

 - Learning performance is *greatly* improved.

 - tl;dr: Generate a curriculum that trades-off difficulty with closeness to target task.


## Leverage the Average: an Analysis of KL regularization in RL

Recent RL algorithms make use of KL divergence as a core component and have shown good performance.

 - Study of *why* KL regularization helps.

Update rule of the DQN: You have some experiences, you want to learn the Q

 - The Q-update error propagates across iterations - search for the policy that maximizes q_k
 - Then you update the q by looking at what returns you got when you sample from the new policy.

Regularized Approximate Value Iteration:

 - We can add some entropy regularization - penalizes deterministic policies
 - Solution is the softmax of q_k
 - Penalizes divergence between greedy policy and uniform one
 - Add a KL - penalizes divergence between previous policy and current one.


Why does the KL regularization in TRPO help so much?
 - The greedy step is a legendre-fenchel transform
 - new policy is proportional to the old one times the softmax of the q-value
 - greedy policy will indeed be a softmax over the scaled sum of all past q values

Main contribution is an analysis of this kind of scheme
 - study the propagation of the error
 - the bound is the difference between the optimal q and the q computed by the algorithm
 - regularizing slows down convergence
 - for regularized value iteration - you have the norm of the average of the error
   rather than the sum of error norms
   - this means that your bound is much lower

## Imitation Learning without Policy Optimization

https://arxiv.org/abs/2006.13258

In IL we consider an MDP without a reward function - learn
a policy that mimics the behaviour of the expert.

Two ways:
 - Match the trajectory distribution
 - Match visitation distribution

GAIL (generative adversarial imitation learning):

 - Train a generative distribution that approximates the data distribution
 - Discriminator - classify if state-action pairs are coming from agent or expert
 - Generator - update to reduce the distribution

Update the policy using RL with reward given if you fool the generator
 - whole thing relies on the reward

This work:
 - Structued discrimiantor
   - define the optimal discrimiantor for any given distributor
   - $D*(x) = \frac{p_d(x)}{p_d(x) + p_G(x)}$
   - Instead of directly parameterizing the discriminator, parameterize
     $\tilde p$, from which we parameterize $\tilde D^*$ (so generator distribution given)
   - Environment terms cancel out - structure discriminator is given entirely
     by the policy distributions
   - Optimal discriminator matches the trajectory distribution of the expert.
   - Discriminator must be maximally confused by the generator - we update the
     generator for free

 - However, this relies on the idea that the discriminator is up to date

   - So ASAF:
    - Given some expert trajectories
    - Randomly initialize $\pi$ and set $\pi_G \to \bar \pi$
    - For some steps, collect trajectories using $\pi_G$ and update $\bar \pi$
      by minimizing BCE loss with structured discriminator
    - Update generated policy $\pi_g = \bar \pi$
    - Instead of updating the generator policy from RL, we just set it directly

   - Two hyperparameters:
    - Window size
    - Window stride

## Reward Propagation using Graph Convolutional Networks

 - typically we augment the reward function with reward shaping
   - intrinsic motivation
   - uncertainty detetection
 - changing the reward function might change the policy

 - reward shaping function should take the form: $\gamma \phi(s') - \phi(s)$
 - how to define this function?

this work: propagate information from rewarding states and use it as a potential function

 - option 1: message passing: look at the probabilistic inference view
   - s are states, a are actions, o is a multivariate variable
   - $\beta$: prob of future optimal trajectory
   - $\alpha$: prob of past optimal trajectory
   - multiply them to get probabiltiy that we lie within an optimal trajectory
 - option 2: approximate the solution through parametric modle
   - GCNs
    - Softmax - output of our GCN - define rewarding states
   - Example: 4 rooms, you have to go to some target
     - GCN gives a distribution that we can learn


how does this scale up to large or continuous environments?
 - sample trajectories on which to train the GCN
 - represent 4-rooms as a graph, train GCN on trajectories
   - keep or discard this information

Recap:
 - Propose a way to define potential functions as distribution over states
   by propagating information about rewarding state
 - Leverage GCNs
 - Working on a different level than states such as state abstractions

How to approximate the underlying graph?
 - Model-based rollouts?
 - Grid-cell constructs?
 - Work at a level different from states

## Latent world models for Intrinsically Motivated Explroation

Consider RL exploration problem:

 1. Latent: self-supervised represention learning method, arrange embeddings
    respecting temporal distance of observations
 2. World model: Estimation of missing information about the environment
    related to partial observability in addition to novelty

### Representation learning procedure:
 - Take two close observations, encode them, whiten, minimize MSE
 - the idea with whitening is that reduce any cross-correlation
 - in the whitened domain, the positives are pulled closer together
 - the final aligned representation exists on a sphere

### Latent world model

 - LWM is an RNN optimized to predict $\phi$ embedding of next state given current
   embedding
 - LWM and recurrent DQM are trained jointly
 - Prediction error of LWM for each step of the unroll is used for 
   intrinsic reward
   - intrinsic reward - the more difficult it is to predict the
     next step, the higher the reward

### Partially observable labyrinth

 - Each number indicates if there is a door
 - you can see only the current room
 - visit all rooms
 - if you add some intrinsic reward based on the prediction error
    - you can predict rooms you've already seen, on new rooms you
      have a higher reward

 - source code of this method is available.
