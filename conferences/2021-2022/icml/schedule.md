# ICML Schedule

## Tutorials
Tutorial: Bridging Learning and Decisionmaking: https://icml.cc/virtual/2022/tutorial/18438

Tutorial: Causality and Deep Learning: https://icml.cc/virtual/2022/tutorial/18442

## Workshops

[Spurious correlations, Invariance, and Stability (SCIS)](https://icml.cc/virtual/2022/workshop/13461)

[Principles of Distribution Shift (PODS)](https://icml.cc/virtual/2022/workshop/13465)

## Transformers / Attention

### On the Learning of Non-Autoregressive Transformers

[[on_the_learning_of_non_autoregressive_transformers]]

### DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale
[[deepspeed_moe_advancing_mixture_of_experts_in_inference_and_trianing_to_power_next_generation_ai_scale]]



### CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer




### From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers



### Flowformer: Linearizing Transformers with Conservation Flows

[[flowformer_linearizing_transformers_with_conservation_flows]]

### Improving Transformers with Probabilistic Attention Keys

[[improving_transformers_with_probabilistic_attention_keys]]

### Rethinking Attention-Model Explainability through Faithfulness Violation Test

[[rethinking_attention_model_explainability_through_faithfulness_violation_test]]


### GLaM: Efficient Scaling of Language Models with Mixture-of-Experts

[[glam_efficient_scaling_of_language_models_with_mixture_of_experts]]

### Flow-Guided Sparse Transformer for Video Deblurring

[[flow_guided_sparse_transformer_for_video_deblurring]]
### Staged Training for Transformer Language Models
[[staged_training_for_transformer_language_models]]


### Learning Multiscale Transformer Models for Sequence Generation

[[learning_multiscale_transformer_models_for_sequence_generation]]


## Generalization / OOD
### Certifying Out-of-Domain Generalization for Blackbox Functions

[[certifying_out_of_domain_generalization_for_blackblox_functions]]

## Understanding Dataset Difficulty with V-Usable Information

[[understanding_dataset_difficulty_with_v_usable_information]]


## Improving Out-of-Distribution Robustness via Selective Augmentation
[[improving_out_of_distribution_robustness_via_selective_augmentation]]



## The CLRS Algorithmic Reasoning Benchmark

Comes from the CLRS textbook

Benchmark covers instances of sorting, searching, dynamic programming, graph algorithms and string algorithms.

Tasks requiring numerical outputs were excluded.

Baseline
 - Encode-process-decode.
 - Processors:
	 - [[deep_sets]]
	 - Graph Attention Networks
	 - Message Passing Neural Networks
	 - Pointer Graph Networks
	 - Memory Networks

MPNN/Graph Attention Networks seem to dominate.

## Understanding Robust Generalization in Learning Regular Languages

[[understanding_robust_generalization_in_learning_regular_languages]]

## The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention
[[the_dual_form_of_neural_networks_revisited_connecting_test_time_predictions_to_training_patterns_via_spotlights_of_attention]]


## Contrastive

### data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language

[[data2vec_a_general_framework_for_self_supervised_learning_in_speech_vision_and_language]]


### Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)

[[data_determines_distributional_robustness_in_constrastive_language_image_pre_training]]


### Robustness Verification for Contrastive Learning

[[robustness_verification_for_constrastive_learning]]

### Do More Negative Samples Necessarily Hurt In Contrastive Learning?

### MetAug: Contrastive Learning via Meta Feature Augmentation

[[metaug_contrastive_learning_via_meta_feature_agumentation]]

### Investigating Why Contrastive Learning Benefits Robustness against Label Noise

### Interventional Contrastive Learning with Meta Semantic Regularizer

### On the Surrogate Gap between Contrastive and Supervised Losses

### Exploring the Gap between Collapsed & Whitened Features in Self-Supervised Learning

## Offline RL

### Offline RL Policies Should Be Trained to be Adaptive

### Robust Imitation Learning against Variations in Environment Dynamics

### Discriminator-Weighted Offline Imitation Learning from Suboptimal Demonstrations

### Versatile Offline Imitation from Observations and Examples via Regularized State-Occupancy Matching

### Adversarially Trained Actor Critic for Offline Reinforcement Learning

### Provably Efficient Offline Reinforcement Learning for Partially Observable Markov Decision Processes

### On the Role of Discount Factor in Offline Reinforcement Learning

### Offline Meta-Reinforcement Learning with Online Self-Supervision

### How to Leverage Unlabeled Data in Offline Reinforcement Learning

## Meta-Learning / Optimization

### Efficient Variance Reduction for Meta-learning

### Towards understanding how momentum improves generalization in deep learning

### What Can Linear Interpolation of Neural Network Loss Landscapes Tell Us?

### The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks

### Transformers are Meta-Reinforcement Learners

## Diffusion / Generative

### Latent Diffusion Energy-Based Model for Interpretable Text Modelling

### GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models

### Planning with Diffusion for Flexible Behavior Synthesis

### Style Equalization: Unsupervised Learning of Controllable Generative Sequence Models

## Causal

### Causal Imitation Learning under Temporally Correlated Noise

### Tell me why! Explanations support learning relational and causal structure

## Compositional

### Toward Compositional Generalization in Object-Oriented World Modeling

## RL

### [Prompting Decision Transformer for Few-Shot Policy Generalization](https://icml.cc/virtual/2022/poster/18153)

### Denoised MDPs: Learning World Models Better Than the World Itself

### Fast Population-Based Reinforcement Learning on a Single Machine

### Temporal Difference Learning for Model Predictive Control

### Large Batch Experience Replay

### Asking for Knowledge (AFK): Training RL Agents to Query External Knowledge Using Language

### Online Decision Transformer

### Bisimulation Makes Analogies in Goal-Conditioned Reinforcement Learning

## Vision / Language

### VLUE: A Multi-Task Multi-Dimension Benchmark for Evaluating Vision-Language Pre-training