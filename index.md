# causal
 - [[pdfs/causal/a_meta_transfer_objective_for_learning_to_disentangle_causal_mechanisms.pdf|a_meta_transfer_objective_for_learning_to_disentangle_causal_mechanisms]]: . [[papers/causal/a_meta_transfer_objective_for_learning_to_disentangle_causal_mechanisms.md|Notes]]
 - [[pdfs/causal/indepedendent_causal_mechanisms.pdf|indepedendent_causal_mechanisms]]: . [[papers/causal/indepedendent_causal_mechanisms.md|Notes]]
 - [[pdfs/causal/nonlinear_causal_discovery_additive_noise.pdf|nonlinear_causal_discovery_additive_noise]]: . [[papers/causal/nonlinear_causal_discovery_additive_noise.md|Notes]]
 - [[pdfs/causal/towards_causal_representation_learning.pdf|towards_causal_representation_learning]]: . [[papers/causal/towards_causal_representation_learning.md|Notes]]
 - [[pdfs/causal/video_causal_discovery.pdf|video_causal_discovery]]: . [[papers/causal/video_causal_discovery.md|Notes]]
# cv
 - [[pdfs/cv/film_feature_wise_linear_modulation.pdf|FiLM - Visual Reasoning with a General Conditioning Layer.]]: (cite: conf/aaai/PerezSVDC18) . [[papers/cv/film_feature_wise_linear_modulation.md|Notes]]
 - [[pdfs/cv/mlp_mixer.pdf|MLP-Mixer - An all-MLP Architecture for Vision.]]: Takes the "image-patch transformers" architectures and removes the transformers to test the hypothesis that the patches are what matters. Based exclusively on MLPs. Apply the MLPs to image patches (mixing the per-location features) and then apply the MLPS across patches (mixing spatial information). [[papers/cv/mlp_mixer.md|Notes]]
 - [[pdfs/cv/patches_are_all_you_need_.pdf|Patches Are All You Need?]]: (cite: journals/corr/abs-2201-09792/Trockman/2022) Is the performance of ViTs due to the transformer or due to the patches? Paper proposes ConvMixer. ConvMixer only uses convolutions to achieve the mixing steps. [[papers/cv/patches_are_all_you_need_.md|Notes]]
 - [[pdfs/cv/are_cnn_or_transformers_more_like_human_vision.pdf|are_cnn_or_transformers_more_like_human_vision]]: . [[papers/cv/are_cnn_or_transformers_more_like_human_vision.md|Notes]]
 - [[pdfs/cv/deep_automodulators.pdf|deep_automodulators]]: . [[papers/cv/deep_automodulators.md|Notes]]
 - [[pdfs/cv/object_keypoints.pdf|object_keypoints]]: . [[papers/cv/object_keypoints.md|Notes]]
 - [[pdfs/cv/pretraining_from_pixels.pdf|pretraining_from_pixels]]: . [[papers/cv/pretraining_from_pixels.md|Notes]]
 - [[pdfs/cv/regnet.pdf|regnet]]: . [[papers/cv/regnet.md|Notes]]
 - [[pdfs/cv/residual_attention_network.pdf|residual_attention_network]]: . [[papers/cv/residual_attention_network.md|Notes]]
 - [[pdfs/cv/revisiting_resnets_improved_training_and_scaling.pdf|revisiting_resnets_improved_training_and_scaling]]: . [[papers/cv/revisiting_resnets_improved_training_and_scaling.md|Notes]]
 - [[pdfs/cv/spatial_attention.pdf|spatial_attention]]: . [[papers/cv/spatial_attention.md|Notes]]
 - [[pdfs/cv/unsupervised_learning_of_object_landmarks_conditional_image_generation.pdf|unsupervised_learning_of_object_landmarks_conditional_image_generation]]: . [[papers/cv/unsupervised_learning_of_object_landmarks_conditional_image_generation.md|Notes]]
# dbml
 - [[pdfs/dbml/dbml_survey.pdf|dbml_survey]]: . [[papers/dbml/dbml_survey.md|Notes]]
# dl
 - [[pdfs/dl/bengio_higher_level_cognition.pdf|bengio_higher_level_cognition]]: . [[papers/dl/bengio_higher_level_cognition.md|Notes]]
 - [[pdfs/dl/deconvolution.pdf|deconvolution]]: . [[papers/dl/deconvolution.md|Notes]]
 - [[pdfs/dl/deep_reasoning_networks.pdf|deep_reasoning_networks]]: . [[papers/dl/deep_reasoning_networks.md|Notes]]
 - [[pdfs/dl/deep_sets.pdf|deep_sets]]: . [[papers/dl/deep_sets.md|Notes]]
 - [[pdfs/dl/dl_book_a_ilin.pdf|dl_book_a_ilin]]: . [[papers/dl/dl_book_a_ilin.md|Notes]]
 - [[pdfs/dl/good_init.pdf|good_init]]: . [[papers/dl/good_init.md|Notes]]
 - [[pdfs/dl/hinton_part_whole_hierarchies.pdf|hinton_part_whole_hierarchies]]: . [[papers/dl/hinton_part_whole_hierarchies.md|Notes]]
 - [[pdfs/dl/implicit_mle_backprop_discrete_exponential_family.pdf|implicit_mle_backprop_discrete_exponential_family]]: . [[papers/dl/implicit_mle_backprop_discrete_exponential_family.md|Notes]]
 - [[pdfs/dl/mentornet_corrupted_labels.pdf|mentornet_corrupted_labels]]: . [[papers/dl/mentornet_corrupted_labels.md|Notes]]
 - [[pdfs/dl/neural_tangent_kernel_eigenvalues_accurately_predict_generalization.pdf|neural_tangent_kernel_eigenvalues_accurately_predict_generalization]]: . [[papers/dl/neural_tangent_kernel_eigenvalues_accurately_predict_generalization.md|Notes]]
 - [[pdfs/dl/overparameterization.pdf|overparameterization]]: . [[papers/dl/overparameterization.md|Notes]]
 - [[pdfs/dl/rim.pdf|rim]]: . [[papers/dl/rim.md|Notes]]
 - [[pdfs/dl/slide_lsh.pdf|slide_lsh]]: . [[papers/dl/slide_lsh.md|Notes]]
 - [[pdfs/dl/soft_sort_continuous_argsort.pdf|soft_sort_continuous_argsort]]: . [[papers/dl/soft_sort_continuous_argsort.md|Notes]]
## compositional
 - [[pdfs/dl/compositional/compose_ae_compositional_learning_of_image_text_query_for_image_retrieval.pdf|Compositional Learning of Image-Text Query for Image Retrieval.]]: (cite: conf/wacv/AnwaarLK21) . [[papers/dl/compositional/compose_ae_compositional_learning_of_image_text_query_for_image_retrieval.md|Notes]]
 - [[pdfs/dl/compositional/compositional_generalization_through_meta_seq2seq.pdf|Compositional generalization through meta sequence-to-sequence learning.]]: (cite: conf/nips/Lake19) . [[papers/dl/compositional/compositional_generalization_through_meta_seq2seq.md|Notes]]
 - [[pdfs/dl/compositional/compositionality_decomposed_neural_networks_generalize.pdf|Compositionality Decomposed - How do Neural Networks Generalise?]]: (cite: journals/jair/HupkesDMB20) . [[papers/dl/compositional/compositionality_decomposed_neural_networks_generalize.md|Notes]]
 - [[pdfs/dl/compositional/latent_compositional_representations_improve_systematic_generalization_in_grounded_question_answering.pdf|Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering.]]: (cite: journals/tacl/BoginSGB21) . [[papers/dl/compositional/latent_compositional_representations_improve_systematic_generalization_in_grounded_question_answering.md|Notes]]
 - [[pdfs/dl/compositional/composer_visually_grounded_concept_composition.pdf|Visually Grounded Concept Composition.]]: (cite: conf/emnlp/0002HQSS21) . [[papers/dl/compositional/composer_visually_grounded_concept_composition.md|Notes]]
 - [[pdfs/dl/compositional/visually_grounded_concept_composition.pdf|visually_grounded_concept_composition]]: . [[papers/dl/compositional/visually_grounded_concept_composition.md|Notes]]
## contrastive
 - [[pdfs/dl/contrastive/a_little_help_from_my_friends_nearest_neighbour_contrastive_learning.pdf|a_little_help_from_my_friends_nearest_neighbour_contrastive_learning]]: . [[papers/dl/contrastive/a_little_help_from_my_friends_nearest_neighbour_contrastive_learning.md|Notes]]
 - [[pdfs/dl/contrastive/byol_self_supervised_no_contrastive.pdf|byol_self_supervised_no_contrastive]]: . [[papers/dl/contrastive/byol_self_supervised_no_contrastive.md|Notes]]
 - [[pdfs/dl/contrastive/clip_learning_transferable_visual_models_nl_supervision.pdf|clip_learning_transferable_visual_models_nl_supervision]]: . [[papers/dl/contrastive/clip_learning_transferable_visual_models_nl_supervision.md|Notes]]
 - [[pdfs/dl/contrastive/contrastive_predictive_coding_infonce.pdf|contrastive_predictive_coding_infonce]]: . [[papers/dl/contrastive/contrastive_predictive_coding_infonce.md|Notes]]
 - [[pdfs/dl/contrastive/contrastive_representaiton_hypersphere.pdf|contrastive_representaiton_hypersphere]]: . [[papers/dl/contrastive/contrastive_representaiton_hypersphere.md|Notes]]
 - [[pdfs/dl/contrastive/ermolov_whitening_ssl.pdf|ermolov_whitening_ssl]]: . [[papers/dl/contrastive/ermolov_whitening_ssl.md|Notes]]
 - [[pdfs/dl/contrastive/scaling_up_visual_and_vision_language_representation_learning_noisy_text.pdf|scaling_up_visual_and_vision_language_representation_learning_noisy_text]]: . [[papers/dl/contrastive/scaling_up_visual_and_vision_language_representation_learning_noisy_text.md|Notes]]
 - [[pdfs/dl/contrastive/simclr_contrastive_visual_rep.pdf|simclr_contrastive_visual_rep]]: . [[papers/dl/contrastive/simclr_contrastive_visual_rep.md|Notes]]
## conv
 - [[pdfs/dl/conv/coord_conv.pdf|coord_conv]]: . [[papers/dl/conv/coord_conv.md|Notes]]
## disentanglement
 - [[pdfs/dl/disentanglement/trauble_on_disentangled_representations_learned_from_correlated_data.pdf|On Disentangled Representations Learned from Correlated Data.]]: (cite: conf/icml/TraubleCKLDGSB21) Experimental training setups for disentanglement are far too idealistic, since typically you observe all combinations of factors of variation. Realistic datasets have correlations between factors of variation. This work performs a large scale empirical investigation as to whether current disentanglement methods can still learn good representations when the dataset is correlated. Weaker correlations are easier to disentangle. It may be possible to resolve latent entanglement post-hoc by predicting ground truth labels from entangled dimensions using a few examples. [[papers/dl/disentanglement/trauble_on_disentangled_representations_learned_from_correlated_data.md|Notes]]
 - [[pdfs/dl/disentanglement/on_the_transfer_of_disentangled_representations_in_realistic_settings.pdf|On the Transfer of Disentangled Representations in Realistic Settings.]]: (cite: conf/iclr/DittadiTLWAWBS21) . [[papers/dl/disentanglement/on_the_transfer_of_disentangled_representations_in_realistic_settings.md|Notes]]
 - [[pdfs/dl/disentanglement/definition_disentanglement.pdf|Towards a Definition of Disentangled Representations.]]: (cite: journals/corr/abs-1812-02230/Higgins/2018) . [[papers/dl/disentanglement/definition_disentanglement.md|Notes]]
 - [[pdfs/dl/disentanglement/a_framework_for_the_quantitative_evaluation_of_disentangled_representations.pdf|a_framework_for_the_quantitative_evaluation_of_disentangled_representations]]: . [[papers/dl/disentanglement/a_framework_for_the_quantitative_evaluation_of_disentangled_representations.md|Notes]]
 - [[pdfs/dl/disentanglement/bengio_representation_learning_review_new_perspectives.pdf|bengio_representation_learning_review_new_perspectives]]: . [[papers/dl/disentanglement/bengio_representation_learning_review_new_perspectives.md|Notes]]
 - [[pdfs/dl/disentanglement/beta_tcvae.pdf|beta_tcvae]]: . [[papers/dl/disentanglement/beta_tcvae.md|Notes]]
 - [[pdfs/dl/disentanglement/beta_vae_learning_basic_visual_concepts_with_a_constrained_variational_framework.pdf|beta_vae_learning_basic_visual_concepts_with_a_constrained_variational_framework]]: . [[papers/dl/disentanglement/beta_vae_learning_basic_visual_concepts_with_a_constrained_variational_framework.md|Notes]]
 - [[pdfs/dl/disentanglement/challenging_assumptions_unsupervised_disentanglement.pdf|challenging_assumptions_unsupervised_disentanglement]]: . [[papers/dl/disentanglement/challenging_assumptions_unsupervised_disentanglement.md|Notes]]
 - [[pdfs/dl/disentanglement/disentangled_recurrent_wasserstein_autoencoder.pdf|disentangled_recurrent_wasserstein_autoencoder]]: . [[papers/dl/disentanglement/disentangled_recurrent_wasserstein_autoencoder.md|Notes]]
 - [[pdfs/dl/disentanglement/disentangled_representations_correlated_data.pdf|disentangled_representations_correlated_data]]: . [[papers/dl/disentanglement/disentangled_representations_correlated_data.md|Notes]]
 - [[pdfs/dl/disentanglement/disentangling_by_subspace_diffusion.pdf|disentangling_by_subspace_diffusion]]: . [[papers/dl/disentanglement/disentangling_by_subspace_diffusion.md|Notes]]
 - [[pdfs/dl/disentanglement/factor_vae.pdf|factor_vae]]: . [[papers/dl/disentanglement/factor_vae.md|Notes]]
 - [[pdfs/dl/disentanglement/model_selection_vae_disentanglement.pdf|model_selection_vae_disentanglement]]: . [[papers/dl/disentanglement/model_selection_vae_disentanglement.md|Notes]]
 - [[pdfs/dl/disentanglement/monet_unsupervised_scene_decomposition_representation.pdf|monet_unsupervised_scene_decomposition_representation]]: . [[papers/dl/disentanglement/monet_unsupervised_scene_decomposition_representation.md|Notes]]
 - [[pdfs/dl/disentanglement/nonlinear_disentanglement_temporal_sparse_coding.pdf|nonlinear_disentanglement_temporal_sparse_coding]]: . [[papers/dl/disentanglement/nonlinear_disentanglement_temporal_sparse_coding.md|Notes]]
 - [[pdfs/dl/disentanglement/on_disentangled_representations_learned_from_correlated_data.pdf|on_disentangled_representations_learned_from_correlated_data]]: . [[papers/dl/disentanglement/on_disentangled_representations_learned_from_correlated_data.md|Notes]]
 - [[pdfs/dl/disentanglement/progressive_hierarchical_disentanglement.pdf|progressive_hierarchical_disentanglement]]: . [[papers/dl/disentanglement/progressive_hierarchical_disentanglement.md|Notes]]
 - [[pdfs/dl/disentanglement/sbd_spatial_broadcast_decoder_vae_disentanglement.pdf|sbd_spatial_broadcast_decoder_vae_disentanglement]]: . [[papers/dl/disentanglement/sbd_spatial_broadcast_decoder_vae_disentanglement.md|Notes]]
 - [[pdfs/dl/disentanglement/sbd_symmetry_based_disentangled_representation_interaction.pdf|sbd_symmetry_based_disentangled_representation_interaction]]: . [[papers/dl/disentanglement/sbd_symmetry_based_disentangled_representation_interaction.md|Notes]]
 - [[pdfs/dl/disentanglement/the_role_of_disentanglement_in_generalisation.pdf|the_role_of_disentanglement_in_generalisation]]: . [[papers/dl/disentanglement/the_role_of_disentanglement_in_generalisation.md|Notes]]
 - [[pdfs/dl/disentanglement/udr_unsupervised_model_selection_for_variational_disentangled_representation_learning.pdf|udr_unsupervised_model_selection_for_variational_disentangled_representation_learning]]: . [[papers/dl/disentanglement/udr_unsupervised_model_selection_for_variational_disentangled_representation_learning.md|Notes]]
 - [[pdfs/dl/disentanglement/understanding_disentanglement_beta_vae.pdf|understanding_disentanglement_beta_vae]]: . [[papers/dl/disentanglement/understanding_disentanglement_beta_vae.md|Notes]]
 - [[pdfs/dl/disentanglement/unsupervised_model_selection_for_variational_disentangled_representation_learning.pdf|unsupervised_model_selection_for_variational_disentangled_representation_learning]]: . [[papers/dl/disentanglement/unsupervised_model_selection_for_variational_disentangled_representation_learning.md|Notes]]
 - [[pdfs/dl/disentanglement/vae_does_pca_by_accident.pdf|vae_does_pca_by_accident]]: . [[papers/dl/disentanglement/vae_does_pca_by_accident.md|Notes]]
 - [[pdfs/dl/disentanglement/vq_vae.pdf|vq_vae]]: . [[papers/dl/disentanglement/vq_vae.md|Notes]]
 - [[pdfs/dl/disentanglement/vqvae_training.pdf|vqvae_training]]: . [[papers/dl/disentanglement/vqvae_training.md|Notes]]
 - [[pdfs/dl/disentanglement/weak_supervised_disentanglement_guarantees.pdf|weak_supervised_disentanglement_guarantees]]: . [[papers/dl/disentanglement/weak_supervised_disentanglement_guarantees.md|Notes]]
 - [[pdfs/dl/disentanglement/weakly_supervised_disentanglement.pdf|weakly_supervised_disentanglement]]: . [[papers/dl/disentanglement/weakly_supervised_disentanglement.md|Notes]]
## generative
 - [[pdfs/dl/generative/adversarial_feature_learning.pdf|adversarial_feature_learning]]: . [[papers/dl/generative/adversarial_feature_learning.md|Notes]]
 - [[pdfs/dl/generative/big_bigan.pdf|big_bigan]]: . [[papers/dl/generative/big_bigan.md|Notes]]
 - [[pdfs/dl/generative/big_self_supervised_strong_semi_supervised.pdf|big_self_supervised_strong_semi_supervised]]: . [[papers/dl/generative/big_self_supervised_strong_semi_supervised.md|Notes]]
 - [[pdfs/dl/generative/generative_imagination_object_world_models.pdf|generative_imagination_object_world_models]]: . [[papers/dl/generative/generative_imagination_object_world_models.md|Notes]]
 - [[pdfs/dl/generative/infogan.pdf|infogan]]: . [[papers/dl/generative/infogan.md|Notes]]
 - [[pdfs/dl/generative/mixup_resynthesis.pdf|mixup_resynthesis]]: . [[papers/dl/generative/mixup_resynthesis.md|Notes]]
 - [[pdfs/dl/generative/nash_generating_images_sparse_representations.pdf|nash_generating_images_sparse_representations]]: . [[papers/dl/generative/nash_generating_images_sparse_representations.md|Notes]]
 - [[pdfs/dl/generative/nvae_hierarchical_vae.pdf|nvae_hierarchical_vae]]: . [[papers/dl/generative/nvae_hierarchical_vae.md|Notes]]
 - [[pdfs/dl/generative/pixelcnn.pdf|pixelcnn]]: . [[papers/dl/generative/pixelcnn.md|Notes]]
### diffusion
 - [[pdfs/dl/generative/diffusion/diffusion_models_beat_gan_on_image_synthesis.pdf|Diffusion Models Beat GANs on Image Synthesis.]]: (cite: journals/corr/abs-2105-05233/Dhariwal/2021) Some small changes to diffusion models can beat GAN models on image synthesis tasks (eg, U-Net, adaptive group normalization, conditioning on the gradients of a classifier, conditional reverse noising). [[papers/dl/generative/diffusion/diffusion_models_beat_gan_on_image_synthesis.md|Notes]]
 - [[pdfs/dl/generative/diffusion/score_based_generative_modeling.pdf|Score-Based Generative Modeling through Stochastic Differential Equations.]]: (cite: conf/iclr/0011SKKEP21) Generative modelling problem - we want to turn noise into something from the data distribution. The basic idea is that if you know the gradient of the noise, then you can make a reverse stochastic differential equation. Proposes a predictor-corrector which combines numerical SDE solvers with an MCMC approach and provides a way to do class-conditional generation, image inpainting, etc. [[papers/dl/generative/diffusion/score_based_generative_modeling.md|Notes]]
### energy
 - [[pdfs/dl/generative/energy/comet_unsupervised_learning_of_compositional_energy_concepts.pdf|comet_unsupervised_learning_of_compositional_energy_concepts]]: . [[papers/dl/generative/energy/comet_unsupervised_learning_of_compositional_energy_concepts.md|Notes]]
 - [[pdfs/dl/generative/energy/implicit_generation_with_energy_based_models.pdf|implicit_generation_with_energy_based_models]]: . [[papers/dl/generative/energy/implicit_generation_with_energy_based_models.md|Notes]]
 - [[pdfs/dl/generative/energy/improved_contrastive_divergence_training_of_energy_based_model.pdf|improved_contrastive_divergence_training_of_energy_based_model]]: . [[papers/dl/generative/energy/improved_contrastive_divergence_training_of_energy_based_model.md|Notes]]
### gan
 - [[pdfs/dl/generative/gan/stylegan3.pdf|Alias-Free Generative Adversarial Networks.]]: (cite: journals/corr/abs-2106-12423/Karras/2021) AKA StyleGAN3. Bad signal processing creates a "texture sticking" problem when interpolating between generated images in the latent space. With some fixes in the encoder/decoder architecture to respect signal processing rules, it fixes the aliasing problem leading to a smoother latent space interpolation without texture sticking. [[papers/dl/generative/gan/stylegan3.md|Notes]]
### vae
 - [[pdfs/dl/generative/vae/inductive_bias_vae.pdf|InteL-VAEs - Adding Inductive Biases to Variational Auto-Encoders via Intermediary Latents.]]: (cite: journals/corr/abs-2106-13746/Miao/2021) Incorporate a mapping function into the encoder of a VAE which maps from a unit sphere distribution to separate clusters. Then it is easier to decode from those clusters. The mapping function can depend on the problem, its a kind of structural prior on the data space. [[papers/dl/generative/vae/inductive_bias_vae.md|Notes]]
## graph
 - [[pdfs/dl/graph/find_your_friendly_neighbourhood_graph_attention.pdf|find_your_friendly_neighbourhood_graph_attention]]: . [[papers/dl/graph/find_your_friendly_neighbourhood_graph_attention.md|Notes]]
 - [[pdfs/dl/graph/graph_neural_networks_with_lea.pdf|graph_neural_networks_with_lea]]: . [[papers/dl/graph/graph_neural_networks_with_lea.md|Notes]]
 - [[pdfs/dl/graph/superglue_learning_feature_matching_gnns.pdf|superglue_learning_feature_matching_gnns]]: . [[papers/dl/graph/superglue_learning_feature_matching_gnns.md|Notes]]
 - [[pdfs/dl/graph/yu_deep_latent_graph_matching.pdf|yu_deep_latent_graph_matching]]: . [[papers/dl/graph/yu_deep_latent_graph_matching.md|Notes]]
## logic
 - [[pdfs/dl/logic/neural_arithmetic_units.pdf|neural_arithmetic_units]]: . [[papers/dl/logic/neural_arithmetic_units.md|Notes]]
## meta
 - [[pdfs/dl/meta/meta_attention_networks.pdf|meta_attention_networks]]: . [[papers/dl/meta/meta_attention_networks.md|Notes]]
 - [[pdfs/dl/meta/meta_learning_few_shot_nlp_survey.pdf|meta_learning_few_shot_nlp_survey]]: . [[papers/dl/meta/meta_learning_few_shot_nlp_survey.md|Notes]]
## noncontrastive
 - [[pdfs/dl/noncontrastive/understanding_self_supervised_learning_dynamics_without_contrastive_pairs.pdf|understanding_self_supervised_learning_dynamics_without_contrastive_pairs]]: . [[papers/dl/noncontrastive/understanding_self_supervised_learning_dynamics_without_contrastive_pairs.md|Notes]]
## object_centric
 - [[pdfs/dl/object_centric/slot_attention.pdf|slot_attention]]: . [[papers/dl/object_centric/slot_attention.md|Notes]]
## representation_learning
 - [[pdfs/dl/representation_learning/sketch_embed_net_learning_novel_concepts_by_imitating_drawings.pdf|SketchEmbedNet - Learning Novel Concepts by Imitating Drawings.]]: (cite: conf/icml/WangRZ21) Learn parameters to an RNN which outputs a program used to produce a sketch matching a particular input. The results show that "learning to sketch" produces representations which classify well compared to contrastive learning and demonstrate "conceptual composition" (snowman - circle + square = square snowman) . [[papers/dl/representation_learning/sketch_embed_net_learning_novel_concepts_by_imitating_drawings.md|Notes]]
 - [[pdfs/dl/representation_learning/visual_representation_learning_does_not_generalize_strongly.pdf|Visual Representation Learning Does Not Generalize Strongly Within the Same Domain.]]: (cite: journals/corr/abs-2107-08221/Schott/2021) Recompose, interpolate or extrapolate only existing factors of variation from the training data. Models that learn the correct mechanism should be able to genearlize to this benchmark, but they fail to do so. Almost all models bad at extrapolation except "transfer learning" models. Unsupervised models not very good at concept composition. [[papers/dl/representation_learning/visual_representation_learning_does_not_generalize_strongly.md|Notes]]
 - [[pdfs/dl/representation_learning/cvae_enhances_salient_features.pdf|cvae_enhances_salient_features]]: . [[papers/dl/representation_learning/cvae_enhances_salient_features.md|Notes]]
## satnet
 - [[pdfs/dl/satnet/abductive_knowledge_induction_satnet.pdf|abductive_knowledge_induction_satnet]]: . [[papers/dl/satnet/abductive_knowledge_induction_satnet.md|Notes]]
 - [[pdfs/dl/satnet/satnet_bridging_dl_logical_reasoning_differentiable_sat_solver_lp.pdf|satnet_bridging_dl_logical_reasoning_differentiable_sat_solver_lp]]: . [[papers/dl/satnet/satnet_bridging_dl_logical_reasoning_differentiable_sat_solver_lp.md|Notes]]
 - [[pdfs/dl/satnet/satnet_nlp_application.pdf|satnet_nlp_application]]: . [[papers/dl/satnet/satnet_nlp_application.md|Notes]]
 - [[pdfs/dl/satnet/satnet_symbolic_grounding_assessment.pdf|satnet_symbolic_grounding_assessment]]: . [[papers/dl/satnet/satnet_symbolic_grounding_assessment.md|Notes]]
 - [[pdfs/dl/satnet/techniques_symbolic_grounding_satnet.pdf|techniques_symbolic_grounding_satnet]]: . [[papers/dl/satnet/techniques_symbolic_grounding_satnet.md|Notes]]
## sequence
 - [[pdfs/dl/sequence/mc_lstm.pdf|MC-LSTM - Mass-Conserving LSTM.]]: (cite: conf/icml/HoedtKKHHNHK21) Adjust the architecture of an LSTM such that "mass" cannot appear from nowhere, it must either explicitly enter the system via the input gate and mass leaving the system plus current mass must sum up to the total mass (input + current mass at previous timestep). [[papers/dl/sequence/mc_lstm.md|Notes]]
 - [[pdfs/dl/sequence/conv_lstm.pdf|conv_lstm]]: . [[papers/dl/sequence/conv_lstm.md|Notes]]
 - [[pdfs/dl/sequence/dilated_rnn.pdf|dilated_rnn]]: . [[papers/dl/sequence/dilated_rnn.md|Notes]]
 - [[pdfs/dl/sequence/urlstm_gating.pdf|urlstm_gating]]: . [[papers/dl/sequence/urlstm_gating.md|Notes]]
## sets
 - [[pdfs/dl/sets/dspn_deep_set_prediction.pdf|dspn_deep_set_prediction]]: . [[papers/dl/sets/dspn_deep_set_prediction.md|Notes]]
 - [[pdfs/dl/sets/relational_set_representations.pdf|relational_set_representations]]: . [[papers/dl/sets/relational_set_representations.md|Notes]]
 - [[pdfs/dl/sets/xie_proximal_point.pdf|xie_proximal_point]]: . [[papers/dl/sets/xie_proximal_point.md|Notes]]
## sparse
 - [[pdfs/dl/sparse/graph_induced_sparsity_gini.pdf|Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity.]]: (cite: conf/icml/HendersonCM21) Introduces Batch Representation Orthonormalization and Gini regularization as a mechanism to regularize and induce sparsity in graphs. Gini regularization effectively puts a penalty on the inverse-gini coefficient, eg, $(\sum^n \sum^m \frac{|w_{ij} - w_{ij'}}{2(n^2 - n)\tilde w_i})^{-1}$ . [[papers/dl/sparse/graph_induced_sparsity_gini.md|Notes]]
 - [[pdfs/dl/sparse/learning_sparse_distributions.pdf|Learning Sparse Distributions using Iterative Hard Thresholding.]]: (cite: conf/nips/ZhangKKK19) Use a projected gradient descent algorithm known as Iterative Hard Trehsholding to sparsify distributions.  The proposed greedy algorithm can approximate a sparse distribution nearing the  modelled distribution in density. The GSProj algorithm iteratively solves $j \in \arg \min_{i \in [n] \setminus S} \{\min_{p \in P_{s \cup i}} ||p(\cdot) - q(\cdot)||^2_2\}$ , adding $j$ to $S$ until the desired density level is reached. [[papers/dl/sparse/learning_sparse_distributions.md|Notes]]
 - [[pdfs/dl/sparse/sparse_linear_layers_debuggable_deep_networks.pdf|Leveraging Sparse Linear Layers for Debuggable Deep Networks.]]: (cite: conf/icml/WongSM21) Demonstrates how fitting sparse linear models learned over deep feature representations can lead to more debuggable neural networks. View the network as a "deep feature extractor" and linear "decision layer" and only apply the sparsity metric to the decision layer. In the experiments, the features selected by the sparse decision layers "better summarize the model's decision process". [[papers/dl/sparse/sparse_linear_layers_debuggable_deep_networks.md|Notes]]
 - [[pdfs/dl/sparse/lockout_sparse_regularization_neural_networks.pdf|Lockout: Sparse Reguarlization of Neural Networks]]: (cite: journals/corr/abs-2107-07160/Valdes/2021) Proposes Lockout, a mechanism to perform sparsity regularization on nonlinear hypothesis functions. At every iteration, linearlize the loss function and sparsity constraint, solve the corresponding linear programming problem. Effectively, explicit modification of the solution is performed at each step so as to maintain a desired level of sparsity. Only parameters that are not penalized by the constraint are updated first, then update parameters for which decreasing their absolute value decreases the loss, then update the most important parameters such that their absolute value increases but the loss decreases, then update rest of the parameters which are not important by decreasing their absolute value. [[papers/dl/sparse/lockout_sparse_regularization_neural_networks.md|Notes]]
 - [[pdfs/dl/sparse/controllable_sparse_alternatives_softmax.pdf|On Controllable Sparse Alternatives to Softmax.]]: (cite: conf/nips/LahaCAKSR18) Develops a unified framework for combining sparsemax and softmax with controllable degrees of sparsity. Proposes sparsegen-lin and sparsehourglass. Sparsemax projects the score vector on to a simplex $\rho(z) = \arg \min_{p \in \delta^{k - 1}}||p - z||^2_2 - \lambda ||p||^2_2$ where $g$ is some "transformation function" and $\lambda$ controls the regularization strength.  The experiments show that when using sparsegen as an altenrative to softmax in the attention operator that you get better test set performance. . [[papers/dl/sparse/controllable_sparse_alternatives_softmax.md|Notes]]
## tabular
 - [[pdfs/dl/tabular/regularization_is_all_you_need.pdf|regularization_is_all_you_need]]: . [[papers/dl/tabular/regularization_is_all_you_need.md|Notes]]
## theory
 - [[pdfs/dl/theory/computational_limits_deep_learning.pdf|computational_limits_deep_learning]]: . [[papers/dl/theory/computational_limits_deep_learning.md|Notes]]
 - [[pdfs/dl/theory/modern_math_deep_learning.pdf|modern_math_deep_learning]]: . [[papers/dl/theory/modern_math_deep_learning.md|Notes]]
## transformer
 - [[pdfs/dl/transformer/admin_initialization_understanding_difficulty_training_transformers.pdf|admin_initialization_understanding_difficulty_training_transformers]]: . [[papers/dl/transformer/admin_initialization_understanding_difficulty_training_transformers.md|Notes]]
 - [[pdfs/dl/transformer/deberta_decoding_enhanced_bert.pdf|deberta_decoding_enhanced_bert]]: . [[papers/dl/transformer/deberta_decoding_enhanced_bert.md|Notes]]
 - [[pdfs/dl/transformer/dong_attention_is_not_all_you_need.pdf|dong_attention_is_not_all_you_need]]: . [[papers/dl/transformer/dong_attention_is_not_all_you_need.md|Notes]]
 - [[pdfs/dl/transformer/fnet_mixing_tokens_fourier_transforms.pdf|fnet_mixing_tokens_fourier_transforms]]: . [[papers/dl/transformer/fnet_mixing_tokens_fourier_transforms.md|Notes]]
 - [[pdfs/dl/transformer/insertion_transformer.pdf|insertion_transformer]]: . [[papers/dl/transformer/insertion_transformer.md|Notes]]
 - [[pdfs/dl/transformer/making_transformers_solve_compositional_tasks.pdf|making_transformers_solve_compositional_tasks]]: . [[papers/dl/transformer/making_transformers_solve_compositional_tasks.md|Notes]]
 - [[pdfs/dl/transformer/perceiver_io.pdf|perceiver_io]]: . [[papers/dl/transformer/perceiver_io.md|Notes]]
 - [[pdfs/dl/transformer/pondernet_learning_to_ponder.pdf|pondernet_learning_to_ponder]]: . [[papers/dl/transformer/pondernet_learning_to_ponder.md|Notes]]
 - [[pdfs/dl/transformer/quantifying_attention_flow_transformers.pdf|quantifying_attention_flow_transformers]]: . [[papers/dl/transformer/quantifying_attention_flow_transformers.md|Notes]]
 - [[pdfs/dl/transformer/routing_transfomer.pdf|routing_transfomer]]: . [[papers/dl/transformer/routing_transfomer.md|Notes]]
 - [[pdfs/dl/transformer/self_attention_relative_position_representations.pdf|self_attention_relative_position_representations]]: . [[papers/dl/transformer/self_attention_relative_position_representations.md|Notes]]
 - [[pdfs/dl/transformer/set_transformer.pdf|set_transformer]]: . [[papers/dl/transformer/set_transformer.md|Notes]]
 - [[pdfs/dl/transformer/simple_tricks_to_improve_generalization_of_transformers.pdf|simple_tricks_to_improve_generalization_of_transformers]]: . [[papers/dl/transformer/simple_tricks_to_improve_generalization_of_transformers.md|Notes]]
 - [[pdfs/dl/transformer/t_fixup_improving_transformer_optimization_through_better_initialization.pdf|t_fixup_improving_transformer_optimization_through_better_initialization]]: . [[papers/dl/transformer/t_fixup_improving_transformer_optimization_through_better_initialization.md|Notes]]
 - [[pdfs/dl/transformer/tab_transformer_tabular_data_modeling.pdf|tab_transformer_tabular_data_modeling]]: . [[papers/dl/transformer/tab_transformer_tabular_data_modeling.md|Notes]]
 - [[pdfs/dl/transformer/thinking_like_transformers.pdf|thinking_like_transformers]]: . [[papers/dl/transformer/thinking_like_transformers.md|Notes]]
 - [[pdfs/dl/transformer/transfer_inductive_bias_distillation.pdf|transfer_inductive_bias_distillation]]: . [[papers/dl/transformer/transfer_inductive_bias_distillation.md|Notes]]
### linear
 - [[pdfs/dl/transformer/linear/bigbird.pdf|Big Bird - Transformers for Longer Sequences.]]: (cite: conf/nips/ZaheerGDAAOPRWY20) . [[papers/dl/transformer/linear/bigbird.md|Notes]]
 - [[pdfs/dl/transformer/linear/sparse_transformer.pdf|Generating Long Sequences with Sparse Transformers.]]: (cite: journals/corr/abs-1904-10509/Child/2019) . [[papers/dl/transformer/linear/sparse_transformer.md|Notes]]
 - [[pdfs/dl/transformer/linear/longformer.pdf|Longformer - The Long-Document Transformer.]]: (cite: journals/corr/abs-2004-05150/Beltagy/2020) . [[papers/dl/transformer/linear/longformer.md|Notes]]
 - [[pdfs/dl/transformer/linear/reformer.pdf|Reformer - The Efficient Transformer.]]: (cite: conf/iclr/KitaevKL20) . [[papers/dl/transformer/linear/reformer.md|Notes]]
 - [[pdfs/dl/transformer/linear/adaptive_attention_transformer.pdf|adaptive_attention_transformer]]: . [[papers/dl/transformer/linear/adaptive_attention_transformer.md|Notes]]
 - [[pdfs/dl/transformer/linear/attention_free_transformer.pdf|attention_free_transformer]]: . [[papers/dl/transformer/linear/attention_free_transformer.md|Notes]]
 - [[pdfs/dl/transformer/linear/linformer.pdf|linformer]]: . [[papers/dl/transformer/linear/linformer.md|Notes]]
 - [[pdfs/dl/transformer/linear/liutkus_relative_pos_encoding_linear_complex.pdf|liutkus_relative_pos_encoding_linear_complex]]: . [[papers/dl/transformer/linear/liutkus_relative_pos_encoding_linear_complex.md|Notes]]
 - [[pdfs/dl/transformer/linear/performer.pdf|performer]]: . [[papers/dl/transformer/linear/performer.md|Notes]]
 - [[pdfs/dl/transformer/linear/star_transformer.pdf|star_transformer]]: . [[papers/dl/transformer/linear/star_transformer.md|Notes]]
 - [[pdfs/dl/transformer/linear/transformers_are_rnns.pdf|transformers_are_rnns]]: . [[papers/dl/transformer/linear/transformers_are_rnns.md|Notes]]
### nonparametric
 - [[pdfs/dl/transformer/nonparametric/npt_non_parametric_transformer_self_attention_going_beyond_individual_pairs.pdf|npt_non_parametric_transformer_self_attention_going_beyond_individual_pairs]]: . [[papers/dl/transformer/nonparametric/npt_non_parametric_transformer_self_attention_going_beyond_individual_pairs.md|Notes]]
### positional_encodings
 - [[pdfs/dl/transformer/positional_encodings/conditional_positional_encodings_vision_transformers.pdf|conditional_positional_encodings_vision_transformers]]: . [[papers/dl/transformer/positional_encodings/conditional_positional_encodings_vision_transformers.md|Notes]]
 - [[pdfs/dl/transformer/positional_encodings/conditional_positional_encodings_vit.pdf|conditional_positional_encodings_vit]]: . [[papers/dl/transformer/positional_encodings/conditional_positional_encodings_vit.md|Notes]]
 - [[pdfs/dl/transformer/positional_encodings/rethinking_improving_relative_position_encoding_for_vision_transformer.pdf|rethinking_improving_relative_position_encoding_for_vision_transformer]]: . [[papers/dl/transformer/positional_encodings/rethinking_improving_relative_position_encoding_for_vision_transformer.md|Notes]]
 - [[pdfs/dl/transformer/positional_encodings/roformer_transformer_rotary_position_embedding.pdf|roformer_transformer_rotary_position_embedding]]: . [[papers/dl/transformer/positional_encodings/roformer_transformer_rotary_position_embedding.md|Notes]]
 - [[pdfs/dl/transformer/positional_encodings/self_attention_relative_positions.pdf|self_attention_relative_positions]]: . [[papers/dl/transformer/positional_encodings/self_attention_relative_positions.md|Notes]]
### sparse
 - [[pdfs/dl/transformer/sparse/attention_with_sparsity_regularization.pdf|Attention With Sparsity Regularization for Neural Machine Translation and Summarization.]]: (cite: journals/taslp/ZhangZLZ19) Proposes a sparse attention model in which the sparsity regularization term is designed to augment the objective function. Two kinds of regularization explored: $L_{\infty}$  and entropy minimization. The L1 norm is not really suitable for regularizing the attention mask because it is always 1 (note: you can use it on the outer product before the attention mask is computed!). The $L_{\infty}$ norm regularizer effectively maximizes the biggest value and penalizes everything else. The minimum entropy regularizer takes an information theoretic approach and tries to make the attention distribution as sparse as possible . [[papers/dl/transformer/sparse/attention_with_sparsity_regularization.md|Notes]]
 - [[pdfs/dl/transformer/sparse/top_kast_pascanu.pdf|Top-KAST - Top-K Always Sparse Training.]]: (cite: conf/nips/JayakumarPROE20) Proposes a method that maintains constant sparsity throughout training. Also demonstrates how sparser versions of common architectures can run with significantly fewer resources. It does not require computing the dense parameters first and then sparsifying later. Instead, it consists of selecting some subset of the parameters that correspond to the top-K parameters by parameter magnitude for each training step, then applying gradient steps to a superset of those. Then there's an auxiliary exploration loss to encourage the sparsity mask to adapt during training. To put it another way, you still keep all those parameters in memory, but you select one set to use for the foward pass and a superset of those to update using gradient descent. [[papers/dl/transformer/sparse/top_kast_pascanu.md|Notes]]
### vision
 - [[pdfs/dl/transformer/vision/image_worth_16x16_words_transformers_for_image_recognition_at_scale.pdf|An Image is Worth 16x16 Words - Transformers for Image Recognition at Scale.]]: (cite: conf/iclr/DosovitskiyB0WZ21) First study to show that we can make use of a pure transformer network without baseline convolutions. Studies the scaling properties, in particular transfer learning properties when used with different pre-training dataset sizes. Argues that inductive bias stops mattering at the point where you have tons of data. [[papers/dl/transformer/vision/image_worth_16x16_words_transformers_for_image_recognition_at_scale.md|Notes]]
 - [[pdfs/dl/transformer/vision/do_vision_transformers_see_like_cnns.pdf|do_vision_transformers_see_like_cnns]]: . [[papers/dl/transformer/vision/do_vision_transformers_see_like_cnns.md|Notes]]
 - [[pdfs/dl/transformer/vision/robustness_vision_transformers.pdf|robustness_vision_transformers]]: . [[papers/dl/transformer/vision/robustness_vision_transformers.md|Notes]]
### vision_text
 - [[pdfs/dl/transformer/vision_text/causal_attention_catt.pdf|causal_attention_catt]]: . [[papers/dl/transformer/vision_text/causal_attention_catt.md|Notes]]
 - [[pdfs/dl/transformer/vision_text/clip_transferable_visual_models_from_natural_language_supervision.pdf|clip_transferable_visual_models_from_natural_language_supervision]]: . [[papers/dl/transformer/vision_text/clip_transferable_visual_models_from_natural_language_supervision.md|Notes]]
 - [[pdfs/dl/transformer/vision_text/crrn_cross_modal_relational_reasoning_network_vqa.pdf|crrn_cross_modal_relational_reasoning_network_vqa]]: . [[papers/dl/transformer/vision_text/crrn_cross_modal_relational_reasoning_network_vqa.md|Notes]]
 - [[pdfs/dl/transformer/vision_text/dall_e.pdf|dall_e]]: . [[papers/dl/transformer/vision_text/dall_e.md|Notes]]
 - [[pdfs/dl/transformer/vision_text/global_fusion_attention_for_vision_and_language_understanding.pdf|global_fusion_attention_for_vision_and_language_understanding]]: . [[papers/dl/transformer/vision_text/global_fusion_attention_for_vision_and_language_understanding.md|Notes]]
 - [[pdfs/dl/transformer/vision_text/lxmert.pdf|lxmert]]: . [[papers/dl/transformer/vision_text/lxmert.md|Notes]]
 - [[pdfs/dl/transformer/vision_text/uniter.pdf|uniter]]: . [[papers/dl/transformer/vision_text/uniter.md|Notes]]
## uncertainty
 - [[pdfs/dl/uncertainty/uncertainty_duq.pdf|uncertainty_duq]]: . [[papers/dl/uncertainty/uncertainty_duq.md|Notes]]
# ethics
 - [[pdfs/ethics/ai_and_global_south.pdf|ai_and_global_south]]: . [[papers/ethics/ai_and_global_south.md|Notes]]
 - [[pdfs/ethics/ai_now_2017.pdf|ai_now_2017]]: . [[papers/ethics/ai_now_2017.md|Notes]]
 - [[pdfs/ethics/ai_now_2019.pdf|ai_now_2019]]: . [[papers/ethics/ai_now_2019.md|Notes]]
 - [[pdfs/ethics/bottom_of_the_data_pyramid.pdf|bottom_of_the_data_pyramid]]: . [[papers/ethics/bottom_of_the_data_pyramid.md|Notes]]
 - [[pdfs/ethics/data_feminism.pdf|data_feminism]]: . [[papers/ethics/data_feminism.md|Notes]]
 - [[pdfs/ethics/energy_policy_considerations.pdf|energy_policy_considerations]]: . [[papers/ethics/energy_policy_considerations.md|Notes]]
 - [[pdfs/ethics/hitchhikers_guide_1.pdf|hitchhikers_guide_1]]: . [[papers/ethics/hitchhikers_guide_1.md|Notes]]
 - [[pdfs/ethics/hitchhikers_guide_2.pdf|hitchhikers_guide_2]]: . [[papers/ethics/hitchhikers_guide_2.md|Notes]]
 - [[pdfs/ethics/hitchhikers_guide_3.pdf|hitchhikers_guide_3]]: . [[papers/ethics/hitchhikers_guide_3.md|Notes]]
 - [[pdfs/ethics/india_case_studies.pdf|india_case_studies]]: . [[papers/ethics/india_case_studies.md|Notes]]
 - [[pdfs/ethics/litigatingalgorithms-2019-us.pdf|litigatingalgorithms-2019-us]]: . [[papers/ethics/litigatingalgorithms-2019-us.md|Notes]]
 - [[pdfs/ethics/politics_facial_recognition.pdf|politics_facial_recognition]]: . [[papers/ethics/politics_facial_recognition.md|Notes]]
 - [[pdfs/ethics/situation_method_in_magic.pdf|situation_method_in_magic]]: . [[papers/ethics/situation_method_in_magic.md|Notes]]
## law
 - [[pdfs/ethics/law/ec-ai-regs-annexes.pdf|ec-ai-regs-annexes]]: . [[papers/ethics/law/ec-ai-regs-annexes.md|Notes]]
 - [[pdfs/ethics/law/ec-ai-regs-proposal.pdf|ec-ai-regs-proposal]]: . [[papers/ethics/law/ec-ai-regs-proposal.md|Notes]]
# explainability
 - [[pdfs/explainability/parameter_space_saliency_maps_explainability.pdf|parameter_space_saliency_maps_explainability]]: . [[papers/explainability/parameter_space_saliency_maps_explainability.md|Notes]]
# gnn
 - [[pdfs/gnn/gnn_learnable_structural_and_positional_representations.pdf|Graph Neural Networks with Learnable Structural and Positional Representations.]]: (cite: journals/corr/abs-2110-07875/Dwivedi/2021) Have a separate learned position encoding plus position initializations based on $k$-hop random-walk features. [[papers/gnn/gnn_learnable_structural_and_positional_representations.md|Notes]]
 - [[pdfs/gnn/deep_path_knowledge_graph_reasoning.pdf|deep_path_knowledge_graph_reasoning]]: . [[papers/gnn/deep_path_knowledge_graph_reasoning.md|Notes]]
 - [[pdfs/gnn/gnn_explainer.pdf|gnn_explainer]]: . [[papers/gnn/gnn_explainer.md|Notes]]
 - [[pdfs/gnn/graph_star_net.pdf|graph_star_net]]: . [[papers/gnn/graph_star_net.md|Notes]]
 - [[pdfs/gnn/relational_gcn.pdf|relational_gcn]]: . [[papers/gnn/relational_gcn.md|Notes]]
## applications
### physics
 - [[pdfs/gnn/applications/physics/simulate_physics_gnn.pdf|simulate_physics_gnn]]: . [[papers/gnn/applications/physics/simulate_physics_gnn.md|Notes]]
 - [[pdfs/gnn/applications/physics/symbolic_physics_gnn.pdf|symbolic_physics_gnn]]: . [[papers/gnn/applications/physics/symbolic_physics_gnn.md|Notes]]
 - [[pdfs/gnn/applications/physics/symbolic_regression_graph_inductive_bias.pdf|symbolic_regression_graph_inductive_bias]]: . [[papers/gnn/applications/physics/symbolic_regression_graph_inductive_bias.md|Notes]]
# loss
 - [[pdfs/loss/unbiased_loss_functions.pdf|unbiased_loss_functions]]: . [[papers/loss/unbiased_loss_functions.md|Notes]]
# math
 - [[pdfs/math/dl_derivative_solver_symbolic_seq2seq.pdf|dl_derivative_solver_symbolic_seq2seq]]: . [[papers/math/dl_derivative_solver_symbolic_seq2seq.md|Notes]]
 - [[pdfs/math/nau.pdf|nau]]: . [[papers/math/nau.md|Notes]]
# meta
 - [[pdfs/meta/concept_learners.pdf|concept_learners]]: . [[papers/meta/concept_learners.md|Notes]]
 - [[pdfs/meta/meta_few_shot_nlp.pdf|meta_few_shot_nlp]]: . [[papers/meta/meta_few_shot_nlp.md|Notes]]
 - [[pdfs/meta/tasknorm_meta_learning.pdf|tasknorm_meta_learning]]: . [[papers/meta/tasknorm_meta_learning.md|Notes]]
# nlp
 - [[pdfs/nlp/layout_lm_doc_understanding.pdf|layout_lm_doc_understanding]]: . [[papers/nlp/layout_lm_doc_understanding.md|Notes]]
 - [[pdfs/nlp/prado_on_device_doc_classification.pdf|prado_on_device_doc_classification]]: . [[papers/nlp/prado_on_device_doc_classification.md|Notes]]
 - [[pdfs/nlp/transferrable_visual_models_from_nl_supervision.pdf|transferrable_visual_models_from_nl_supervision]]: . [[papers/nlp/transferrable_visual_models_from_nl_supervision.md|Notes]]
 - [[pdfs/nlp/vqa.pdf|vqa]]: . [[papers/nlp/vqa.md|Notes]]
## data_augmentation
 - [[pdfs/nlp/data_augmentation/coda_data_augmentation_in_nlp.pdf|coda_data_augmentation_in_nlp]]: . [[papers/nlp/data_augmentation/coda_data_augmentation_in_nlp.md|Notes]]
 - [[pdfs/nlp/data_augmentation/grondahl_combinatorial_paraphrasing.pdf|grondahl_combinatorial_paraphrasing]]: . [[papers/nlp/data_augmentation/grondahl_combinatorial_paraphrasing.md|Notes]]
 - [[pdfs/nlp/data_augmentation/grondal_text_oversampling_aug.pdf|grondal_text_oversampling_aug]]: . [[papers/nlp/data_augmentation/grondal_text_oversampling_aug.md|Notes]]
## graph
 - [[pdfs/nlp/graph/gnn_interpreting_nlp_edge_masking.pdf|gnn_interpreting_nlp_edge_masking]]: . [[papers/nlp/graph/gnn_interpreting_nlp_edge_masking.md|Notes]]
## image_captioning
 - [[pdfs/nlp/image_captioning/film.pdf|film]]: . [[papers/nlp/image_captioning/film.md|Notes]]
 - [[pdfs/nlp/image_captioning/kim_vilt_vision_language_transformer_without_convolution.pdf|kim_vilt_vision_language_transformer_without_convolution]]: . [[papers/nlp/image_captioning/kim_vilt_vision_language_transformer_without_convolution.md|Notes]]
 - [[pdfs/nlp/image_captioning/scan_hierarchical_compositional_visual_concepts.pdf|scan_hierarchical_compositional_visual_concepts]]: . [[papers/nlp/image_captioning/scan_hierarchical_compositional_visual_concepts.md|Notes]]
 - [[pdfs/nlp/image_captioning/show_attend_tell.pdf|show_attend_tell]]: . [[papers/nlp/image_captioning/show_attend_tell.md|Notes]]
 - [[pdfs/nlp/image_captioning/text_neural_operator_image_manip.pdf|text_neural_operator_image_manip]]: . [[papers/nlp/image_captioning/text_neural_operator_image_manip.md|Notes]]
## knowledge_graph
 - [[pdfs/nlp/knowledge_graph/glomo_unsupervised_learning_of_transferrable_relational_graphs.pdf|glomo_unsupervised_learning_of_transferrable_relational_graphs]]: . [[papers/nlp/knowledge_graph/glomo_unsupervised_learning_of_transferrable_relational_graphs.md|Notes]]
 - [[pdfs/nlp/knowledge_graph/interpreting_knowledge_graph_relation_representations_from_word_embeddings.pdf|interpreting_knowledge_graph_relation_representations_from_word_embeddings]]: . [[papers/nlp/knowledge_graph/interpreting_knowledge_graph_relation_representations_from_word_embeddings.md|Notes]]
 - [[pdfs/nlp/knowledge_graph/language_models_knowledge_bases.pdf|language_models_knowledge_bases]]: . [[papers/nlp/knowledge_graph/language_models_knowledge_bases.md|Notes]]
 - [[pdfs/nlp/knowledge_graph/seq2kg_e2e_knowledge_graph_creation.pdf|seq2kg_e2e_knowledge_graph_creation]]: . [[papers/nlp/knowledge_graph/seq2kg_e2e_knowledge_graph_creation.md|Notes]]
## rnn
 - [[pdfs/nlp/rnn/neural_machine_translation_jointly_learning_to_align_and_translate.pdf|neural_machine_translation_jointly_learning_to_align_and_translate]]: . [[papers/nlp/rnn/neural_machine_translation_jointly_learning_to_align_and_translate.md|Notes]]
## transformer
 - [[pdfs/nlp/transformer/vaswani_attention.pdf|Attention is All you Need.]]: (cite: conf/nips/VaswaniSPUJGKP17) Introduces the Transformer Architecture, a fully-attentional sequence-to-sequence processing architecture with positional encodings. [[papers/nlp/transformer/vaswani_attention.md|Notes]]
 - [[pdfs/nlp/transformer/gpt3.pdf|Language Models are Few-Shot Learners.]]: (cite: conf/nips/BrownMRSKDNSSAA20) . [[papers/nlp/transformer/gpt3.md|Notes]]
 - [[pdfs/nlp/transformer/transformer_training_tips.pdf|Training Tips for the Transformer Model.]]: (cite: journals/pbml/PopelB18) . [[papers/nlp/transformer/transformer_training_tips.md|Notes]]
# programming
 - [[pdfs/programming/catala_programming_for_law.pdf|catala_programming_for_law]]: . [[papers/programming/catala_programming_for_law.md|Notes]]
# rl
 - [[pdfs/rl/deep_rl_statistical_precipice.pdf|Deep Reinforcement Learning at the Edge of the Statistical Precipice.]]: (cite: journals/corr/abs-2108-13264/Agarwal/2021) Presents a common and robust methodology for reporting scores/improvements using a "handful of runs". [[papers/rl/deep_rl_statistical_precipice.md|Notes]]
 - [[pdfs/rl/embodied_intelligence_via_learning_and_evolution.pdf|Embodied Intelligence via Learning and Evolution.]]: (cite: journals/corr/abs-2102-02202/Gupta/2021) Combines evolutionary algorithms and reinforcement learning to study meta-reinforcement-learning via evolution. [[papers/rl/embodied_intelligence_via_learning_and_evolution.md|Notes]]
 - [[pdfs/rl/act2vec.pdf|act2vec]]: . [[papers/rl/act2vec.md|Notes]]
 - [[pdfs/rl/agent57_atari.pdf|agent57_atari]]: . [[papers/rl/agent57_atari.md|Notes]]
 - [[pdfs/rl/bisimulation_metrics_are_optimal_value_functions.pdf|bisimulation_metrics_are_optimal_value_functions]]: . [[papers/rl/bisimulation_metrics_are_optimal_value_functions.md|Notes]]
 - [[pdfs/rl/efficientzero_mastering_atari_limited_data.pdf|efficientzero_mastering_atari_limited_data]]: . [[papers/rl/efficientzero_mastering_atari_limited_data.md|Notes]]
 - [[pdfs/rl/estimating_disentangled_belief_meta_rl.pdf|estimating_disentangled_belief_meta_rl]]: . [[papers/rl/estimating_disentangled_belief_meta_rl.md|Notes]]
 - [[pdfs/rl/flambe_representation_low_rank_mdp.pdf|flambe_representation_low_rank_mdp]]: . [[papers/rl/flambe_representation_low_rank_mdp.md|Notes]]
 - [[pdfs/rl/hindsight_credit_assignment.pdf|hindsight_credit_assignment]]: . [[papers/rl/hindsight_credit_assignment.md|Notes]]
 - [[pdfs/rl/hypothesis_verification.pdf|hypothesis_verification]]: . [[papers/rl/hypothesis_verification.md|Notes]]
 - [[pdfs/rl/laser_latent_action_space_efficient_rl.pdf|laser_latent_action_space_efficient_rl]]: . [[papers/rl/laser_latent_action_space_efficient_rl.md|Notes]]
 - [[pdfs/rl/predicting_predictable_frames.pdf|predicting_predictable_frames]]: . [[papers/rl/predicting_predictable_frames.md|Notes]]
 - [[pdfs/rl/shortest_path_constrained_rl_sparse_reward.pdf|shortest_path_constrained_rl_sparse_reward]]: . [[papers/rl/shortest_path_constrained_rl_sparse_reward.md|Notes]]
## critical_states
 - [[pdfs/rl/critical_states/world_model_graph.pdf|World Model as a Graph - Learning Latent Landmarks for Planning.]]: (cite: conf/icml/Zhang0S21a) Cluster seen states in the latent space, classify landmarks, induce a graph based on a reachability metric and then plan using the graph.  [[papers/rl/critical_states/world_model_graph.md|Notes]]
 - [[pdfs/rl/critical_states/critical_probabilistic_roadmaps.pdf|critical_probabilistic_roadmaps]]: . [[papers/rl/critical_states/critical_probabilistic_roadmaps.md|Notes]]
 - [[pdfs/rl/critical_states/critical_regions_saliency_maps_planning.pdf|critical_regions_saliency_maps_planning]]: . [[papers/rl/critical_states/critical_regions_saliency_maps_planning.md|Notes]]
## curriculum
 - [[pdfs/rl/curriculum/curriculum_learning_survey.pdf|curriculum_learning_survey]]: . [[papers/rl/curriculum/curriculum_learning_survey.md|Notes]]
 - [[pdfs/rl/curriculum/probabilistic_self_based_learning_with_applications_to_rl.pdf|probabilistic_self_based_learning_with_applications_to_rl]]: . [[papers/rl/curriculum/probabilistic_self_based_learning_with_applications_to_rl.md|Notes]]
## exploration
 - [[pdfs/rl/exploration/is_curiousity_all_you_need.pdf|Is Curiousity all you need? On the utility of emergent behaviours from curious exploration]]: Curiousity in exploration might overwrite useful behaviours used to reach areas of the state space which are now no longer "interesting", meaning that as an agent explores, it forgets how to reach areas of the state space that it did in the past. This paper proposes a mechanism to try and remember those behaviours, so the the entire state space can still be reached. [[papers/rl/exploration/is_curiousity_all_you_need.md|Notes]]
 - [[pdfs/rl/exploration/sample_efficient_rl_pomdp.pdf|Sample-Efficient Reinforcement Learning of Undercomplete POMDPs.]]: (cite: conf/nips/JinKKL20) Shows that partial observability does not preclude efficient reinforcement learning for a rich and interesting subclass of POMDPs. Presents a sample-efficient algorithm called *OOM-UCB* for episodic finite *undercomplete* POMDPs where the number of observation is larger tha the number of latent states and where exploration is essential for learning. OOM-UCB gets an $\epsilon$-optimal sample complexity of $\bar{\mathcal{O}(\frac{1}{\epsilon^2}})$ . [[papers/rl/exploration/sample_efficient_rl_pomdp.md|Notes]]
 - [[pdfs/rl/exploration/curl_contrastive_rl.pdf|curl_contrastive_rl]]: . [[papers/rl/exploration/curl_contrastive_rl.md|Notes]]
 - [[pdfs/rl/exploration/episodic_curiousity_through_reachability.pdf|episodic_curiousity_through_reachability]]: . [[papers/rl/exploration/episodic_curiousity_through_reachability.md|Notes]]
 - [[pdfs/rl/exploration/ex2_exploration_exemplar_models.pdf|ex2_exploration_exemplar_models]]: . [[papers/rl/exploration/ex2_exploration_exemplar_models.md|Notes]]
 - [[pdfs/rl/exploration/imagine_language_cognitive_exploration.pdf|imagine_language_cognitive_exploration]]: . [[papers/rl/exploration/imagine_language_cognitive_exploration.md|Notes]]
 - [[pdfs/rl/exploration/learned_intrinsic_rewards.pdf|learned_intrinsic_rewards]]: . [[papers/rl/exploration/learned_intrinsic_rewards.md|Notes]]
 - [[pdfs/rl/exploration/novelty_search_representational_space.pdf|novelty_search_representational_space]]: . [[papers/rl/exploration/novelty_search_representational_space.md|Notes]]
 - [[pdfs/rl/exploration/provably_efficient_unsupervised.pdf|provably_efficient_unsupervised]]: . [[papers/rl/exploration/provably_efficient_unsupervised.md|Notes]]
 - [[pdfs/rl/exploration/ready_policy_one.pdf|ready_policy_one]]: . [[papers/rl/exploration/ready_policy_one.md|Notes]]
 - [[pdfs/rl/exploration/rig_imagined_goals.pdf|rig_imagined_goals]]: . [[papers/rl/exploration/rig_imagined_goals.md|Notes]]
 - [[pdfs/rl/exploration/self_paced_deep_reinforcement_learning.pdf|self_paced_deep_reinforcement_learning]]: . [[papers/rl/exploration/self_paced_deep_reinforcement_learning.md|Notes]]
## generalization
 - [[pdfs/rl/generalization/survey_generalization_deep_rl_zhang.pdf|A Survey of Generalization in Deep Reinforcement Learning]]: (cite: journals/corr/abs-2111-09794/Kirk/2021) Covers a few different areas, including generalization test environments, metrics for generalization, different types of generalization, un-explored problems in generalization etc. [[papers/rl/generalization/survey_generalization_deep_rl_zhang.md|Notes]]
 - [[pdfs/rl/generalization/neuro_algorithmic_policies_fast_combinatorial_generalization.pdf|Neuro-algorithmic Policies Enable Fast Combinatorial Generalization.]]: (cite: conf/icml/PRM21) . [[papers/rl/generalization/neuro_algorithmic_policies_fast_combinatorial_generalization.md|Notes]]
 - [[pdfs/rl/generalization/neuroevolution_self_interpretable_agents.pdf|Neuroevolution of self-interpretable agents.]]: (cite: conf/gecco/TangNH20) Chop the image up into patches and use attention to select the K most important pathces as a form of sparsity. Agents using this information were robust to changes in the observation such as color perturbations, vertical bars in the input and other distractors in the input. Also showed that such an inductive bias can in some cases be harmful, eg, showed that adding a distractor that was similar to the attended object harmed performance. [[papers/rl/generalization/neuroevolution_self_interpretable_agents.md|Notes]]
 - [[pdfs/rl/generalization/schema_networks_zero_shot_transfer_generative_causal_model.pdf|Schema Networks - Zero-shot Transfer with a Generative Causal Model of Intuitive Physics.]]: (cite: conf/icml/KanskySMELLDSPG17) Parse input into "entities". A schema is a variable associated with a particular entity-attribute and the next timestep. When all the preconditions are satisfied, the schema is active. Has AND and OR gates for the variables. Use linear programming to solve for the schemas. Paper studies the effectiveness of schema networks in the transfer learning setting, with the hypothesis that if a model learns the dynamics from one variation, the others could be played perfetly by planning. [[papers/rl/generalization/schema_networks_zero_shot_transfer_generative_causal_model.md|Notes]]
 - [[pdfs/rl/generalization/sparse_attention_guided_dynamic_value_estimation.pdf|Sparse Attention Guided Dynamic Value Estimation for Single-Task Multi-Scene Reinforcement Learning.]]: (cite: journals/corr/abs-2102-07266/Singh/2021) Multi-scene RL might improve generalization, but if you re-use the value function for many different scenes you get significant variance in the value function estimate. In reality, the features tend to form clusters, so you can estimate the value function more effectively if you know which cluster you belong to. This work introduces a "attention-based" value function, which basically learns some attention weights to figure out which cluster to pay attention to. The work goes further to introduce a method to *sparsify* that attention over the course of training, with a loss function to ensure that there is both sparsity *and* diversity in terms of chosen clusters over the entire dataset. The authors show through their experiments that the sparse attention model used to estimate value leads to policies that significantly outperform a baseline CNN/LSTM/PPO, and furthermore that sparsity actually helps significantly in this area. [[papers/rl/generalization/sparse_attention_guided_dynamic_value_estimation.md|Notes]]
 - [[pdfs/rl/generalization/illuminating_generalization_in_deep_rl_through_procedural_generation.pdf|illuminating_generalization_in_deep_rl_through_procedural_generation]]: . [[papers/rl/generalization/illuminating_generalization_in_deep_rl_through_procedural_generation.md|Notes]]
 - [[pdfs/rl/generalization/measuring_and_characterizing_generalization_in_deep_rl.pdf|measuring_and_characterizing_generalization_in_deep_rl]]: . [[papers/rl/generalization/measuring_and_characterizing_generalization_in_deep_rl.md|Notes]]
 - [[pdfs/rl/generalization/on_the_measure_of_intelligence.pdf|on_the_measure_of_intelligence]]: . [[papers/rl/generalization/on_the_measure_of_intelligence.md|Notes]]
 - [[pdfs/rl/generalization/quantifying_generalization_in_rl.pdf|quantifying_generalization_in_rl]]: . [[papers/rl/generalization/quantifying_generalization_in_rl.md|Notes]]
## graph_based
 - [[pdfs/rl/graph_based/graph_based_generation_of_abstractions.pdf|graph_based_generation_of_abstractions]]: . [[papers/rl/graph_based/graph_based_generation_of_abstractions.md|Notes]]
## grounded_language
 - [[pdfs/rl/grounded_language/survey_rl_informed_by_natural_language.pdf|A Survey of Reinforcement Learning Informed by Natural Language.]]: (cite: conf/ijcai/LuketinaNFFAGWR19) . [[papers/rl/grounded_language/survey_rl_informed_by_natural_language.md|Notes]]
 - [[pdfs/rl/grounded_language/lc_rl_from_language_to_goals.pdf|From Language to Goals - Inverse Reinforcement Learning for Vision-Based Instruction Following.]]: (cite: conf/iclr/FuKLG19) Extend MaxEntIRL, conditions the likelihood prediction of expert trajectories on the language statement given. [[papers/rl/grounded_language/lc_rl_from_language_to_goals.md|Notes]]
 - [[pdfs/rl/grounded_language/gated_attention_architectures_for_task_oriented_language_grounding.pdf|Gated-Attention Architectures for Task-Oriented Language Grounding.]]: (cite: conf/aaai/ChaplotSPRS18) Examined zero-shot task generalization with language-encoded tasks in vizdoom environment, where some tasks were withheld from the training set. 3D Navigation. Reported 70\%+ accuracy on the "hard" dataset with their gated-attention method. Amount of training data generated not reported, but presumably it is high due to the long training time. Training data included "catch-all" descriptions such as "blue object" as goals, meaning that even if "blue torch" was not in the training set, it may still appear as a target in the training data and thus appear in feature maps generated by the convolutional encoder. [[papers/rl/grounded_language/gated_attention_architectures_for_task_oriented_language_grounding.md|Notes]]
 - [[pdfs/rl/grounded_language/cho_vision_and_language_navigation.pdf|Generative Language-Grounded Policy in Vision-and-Language Navigation with Bayes&apos; Rule.]]: (cite: conf/iclr/KuritaC21) Proposes the use of a language-generative model to select actions in vision-anguage problems. In summary, model the probability of the language sequence given an action and divide by the marginal probability of that language statement given all actions and a hidden state. The idea is that the language is a richer learning signal than the action. [[papers/rl/grounded_language/cho_vision_and_language_navigation.md|Notes]]
 - [[pdfs/rl/grounded_language/grounded_language_learning_fast_slow.pdf|Grounded Language Learning Fast and Slow.]]: (cite: conf/iclr/HillTGWMC21) Uses a re-writable memory to enable fast one-shot generalization to new objects. [[papers/rl/grounded_language/grounded_language_learning_fast_slow.md|Notes]]
 - [[pdfs/rl/grounded_language/gll_simulated_3d_world.pdf|Grounded Language Learning in a Simulated 3D World.]]: (cite: journals/corr/HermannHGWFSSCJ17/Hermann/2017) Presents an agent that learns to interpret language in a simulated 3D environment and is rewarded for correctly executing instructions. The agent "learns to relate linguistic symbols to emergent perceptual representations of its physical surroundings and pertinent sequences of actions". The agent's comprehension extends beyond prior experience enabling it to apply "familiar language to unfamiliar situations". The environment is DeepMind Lab. The authors find that language learning is contingent on a combination of reinforcement and unsupervised learning. The authors find that semantic knowledge acquired generalizes zero-shot to "new situations and new language." Also, the authors find that due to the sparse reward, you really need to use some sort of unsuperivsed auxiliary objective to model the dynamics, otherwise you will not learn anything . The paper finds that word learning is much faster if some other words outside the current training set are already known, because the model has learned to ground words with objects in general.  The paper also studied a colour-shape composition experiment and found good performance on unseen compositions of objects. [[papers/rl/grounded_language/gll_simulated_3d_world.md|Notes]]
 - [[pdfs/rl/grounded_language/hanjie_emma_grounding_language_to_entities_and_dynamics.pdf|Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning.]]: (cite: conf/icml/HanjieZN21) Develops a new environment called "Messenger" to investigate the use of natural language conditioned policies. The environment is set up in a way that there is no supervision on the symbol grounding - the agent only gets a reward if it delivers the message correctly, but it must learn to do the symbol grounding itself. The authors develop a model called EMMA "Entity Mapper with Multi-Modal Attention", which in brief encodes text using BERT, then does attention between the encoded text and the symbol embeddings and re-weights the symbols according to this attention. The idea is that the value function should extract information relevant to the entity's role in the world (eg, "enemy, chasing etc"). The authors found that the model could get good generalization on held-out test games with unseen symbol-role combinations (so the model had seen the symbol before, but not in the role as indicated by the language statement). Also they performed a qualitative analysis where they looked at the attention head for language/objects and found that there was sparsity. [[papers/rl/grounded_language/hanjie_emma_grounding_language_to_entities_and_dynamics.md|Notes]]
 - [[pdfs/rl/grounded_language/interactive_grounded_language_acquisition_and_generalization_in_a_2d_world.pdf|Interactive Grounded Language Acquisition and Generalization in a 2D World.]]: (cite: conf/iclr/YuZX18) Presented a method for sharing concept detection between language grounding and question answering. Examined on both the combinatorial generalization out-of-distribution case and also on learning to interpret sentences with new word combinations. Training data consists of over 1 million distinct sentences and about 3 million training examples to reach peak performance. Also achieved very good out of distribution performance, but possibly explained by the diversity of training data available. [[papers/rl/grounded_language/interactive_grounded_language_acquisition_and_generalization_in_a_2d_world.md|Notes]]
 - [[pdfs/rl/grounded_language/inverse_rl_with_natural_language_goals.pdf|Inverse Reinforcement Learning with Natural Language Goals.]]: (cite: conf/aaai/0006S21) Builds upon AGILE by proposing a method for natural-language goal generation given a trajectory and uses these generated goals as positive examples for the discriminator. They find that such goal relabeling greatly assists in improving the generalization of the discriminator. [[papers/rl/grounded_language/inverse_rl_with_natural_language_goals.md|Notes]]
 - [[pdfs/rl/grounded_language/language_conditioned_imitation_learning_for_robot_manipulation_tasks.pdf|Language-Conditioned Imitation Learning for Robot Manipulation Tasks.]]: . [[papers/rl/grounded_language/language_conditioned_imitation_learning_for_robot_manipulation_tasks.md|Notes]]
 - [[pdfs/rl/grounded_language/agile_learning_to_understanding_goal_specifications_by_modelling_reward.pdf|Learning to Understand Goal Specifications by Modelling Reward.]]: (cite: conf/iclr/BahdanauHLHHKG19) Task-conditioned reward modelling using a discriminator. Designed to be used in conjunction with a model-free method. The discriminator learns to distinguish between goal states for instructions as reached by an expert versus non-rewarding states reached by the agent during the learning process. The purpose of this work is to learn a reward function where it isn't feasisble to provide one in the environment (similar also to ValueDICE, RCE). [[papers/rl/grounded_language/agile_learning_to_understanding_goal_specifications_by_modelling_reward.md|Notes]]
 - [[pdfs/rl/grounded_language/understanding_grounded_language_learning_agents.pdf|Understanding Grounded Language Learning Agents.]]: (cite: journals/corr/abs-1710-09867/Hill/2017) Studies some of the learning dynamics characteristics in the grounded language learning problem. Finds that there is a colour bias when learning new object descriptions, because colours are easier to detect than shapes. Negation is hard to represent if trained with a small amount of data. More words are learned more quickly if the range of words to which the agent is exposed is limited at first and then expanded gradully. Words of different semantic classes are learned at different speeds and represented with features that require different degrees of visual processing depth . [[papers/rl/grounded_language/understanding_grounded_language_learning_agents.md|Notes]]
 - [[pdfs/rl/grounded_language/learn_using_natural_language_for_reward_shaping_in_rl.pdf|Using Natural Language for Reward Shaping in Reinforcement Learning.]]: (cite: conf/ijcai/GoyalNM19) Map from free-form natural language instructions to intermediate rewards based on actions takne by agent. 60% completion rate on Montezuma's Revenge. Descriptions obtained from human annotators. Train a discriminator to determine if a particular sentence is related to or unrelated to the action sequence. Then at test time the agent is given the language description of how to solve the task and picks actions so as to maximize the probability of agreement between the action sequence and the language instructions. The main claim is that you get faster learning performance, since you can use the discriminator output as a kind of intermediate reward to guide learning. [[papers/rl/grounded_language/learn_using_natural_language_for_reward_shaping_in_rl.md|Notes]]
 - [[pdfs/rl/grounded_language/early_word_learning.pdf|early_word_learning]]: . [[papers/rl/grounded_language/early_word_learning.md|Notes]]
 - [[pdfs/rl/grounded_language/grounded_lang_simulated_3d_world.pdf|grounded_lang_simulated_3d_world]]: . [[papers/rl/grounded_language/grounded_lang_simulated_3d_world.md|Notes]]
 - [[pdfs/rl/grounded_language/rtfm_generalizing_by_reading.pdf|rtfm_generalizing_by_reading]]: . [[papers/rl/grounded_language/rtfm_generalizing_by_reading.md|Notes]]
 - [[pdfs/rl/grounded_language/speaker_follower_models.pdf|speaker_follower_models]]: . [[papers/rl/grounded_language/speaker_follower_models.md|Notes]]
 - [[pdfs/rl/grounded_language/text_games_natural_language_actions.pdf|text_games_natural_language_actions]]: . [[papers/rl/grounded_language/text_games_natural_language_actions.md|Notes]]
### babyai
 - [[pdfs/rl/grounded_language/babyai/babyai_with_transformers_kujanpaa.pdf|A Deep Relational Model for Natural Language Understanding]]: . [[papers/rl/grounded_language/babyai/babyai_with_transformers_kujanpaa.md|Notes]]
 - [[pdfs/rl/grounded_language/babyai/babyai.pdf|BabyAI - A Platform to Study the Sample Efficiency of Grounded Language Learning.]]: (cite: conf/iclr/Chevalier-Boisvert19) . [[papers/rl/grounded_language/babyai/babyai.md|Notes]]
 - [[pdfs/rl/grounded_language/babyai/babyai1.1.pdf|BabyAI 1.1.]]: (cite: journals/corr/abs-2007-12770/Hui/2020) . [[papers/rl/grounded_language/babyai/babyai1.1.md|Notes]]
 - [[pdfs/rl/grounded_language/babyai/babyai++.pdf|BabyAI++ - Towards Grounded-Language Learning beyond Memorization.]]: (cite: journals/corr/abs-2004-07200/Cao/2020) . [[papers/rl/grounded_language/babyai/babyai++.md|Notes]]
### environments
 - [[pdfs/rl/grounded_language/environments/gscan_grounded_language_benchmarks.pdf|A Benchmark for Systematic Generalization in Grounded Language Understanding.]]: (cite: conf/nips/RuisABBL20) . [[papers/rl/grounded_language/environments/gscan_grounded_language_benchmarks.md|Notes]]
 - [[pdfs/rl/grounded_language/environments/alfred.pdf|ALFRED - A Benchmark for Interpreting Grounded Instructions for Everyday Tasks.]]: (cite: conf/cvpr/ShridharTGBHMZF20) . [[papers/rl/grounded_language/environments/alfred.md|Notes]]
### generalization
 - [[pdfs/rl/grounded_language/generalization/bcz_zero_shot_task_generalization_language.pdf|BC-Z - Zero-Shot Task Generalization with Robotic Imitation Learning.]]: (cite: conf/corl/JangIKKELLF21) Empircal study of a large-scale interactive imitation learning system that can solve many tasks including tasks not seen during training. A large dataset of 100 manipulation tasks is collected across many robots. At test time the robot can perform 24 unseen manipulation tasks between objects that have never previously appeared together in the same scene. [[papers/rl/grounded_language/generalization/bcz_zero_shot_task_generalization_language.md|Notes]]
 - [[pdfs/rl/grounded_language/generalization/environmental_drivers_of_systematicity_and_generalization.pdf|Environmental drivers of systematicity and generalization in a situated agent.]]: (cite: conf/iclr/HillLSCBMS20) Studied what kind of environments and training data can assist generalization to unseen tasks. Concludes that having access to many frames of richly varying multi-modal observations can improve systematic generalization. [[papers/rl/grounded_language/generalization/environmental_drivers_of_systematicity_and_generalization.md|Notes]]
 - [[pdfs/rl/grounded_language/generalization/language_representations_for_generalization_in_reinforcement_learning.pdf|Language Representations for Generalization in Reinforcement Learning.]]: (cite: conf/acml/Nikolaj0FD21) The basic idea here is to use language actions, selecting tokens from the input to generate the output. The paper finds that "agents using language representations generalize better and could solve tasks with more entities, new entities and more complexity than seen in the training task".  [[papers/rl/grounded_language/generalization/language_representations_for_generalization_in_reinforcement_learning.md|Notes]]
 - [[pdfs/rl/grounded_language/generalization/care_multi_task_rl_context_representations.pdf|Multi-Task Reinforcement Learning with Context-based Representations.]]: (cite: conf/icml/Sodhani0P21) Multi-task setting where tasks are encoded as natural language. Presents the CARE model. Uses a model consisting of a mixture of encoders, where encodings are re-weighted and combined according to attention scores. [[papers/rl/grounded_language/generalization/care_multi_task_rl_context_representations.md|Notes]]
 - [[pdfs/rl/grounded_language/generalization/zero_shot_task_adaptation_using_natural_language.pdf|Zero-shot Task Adaptation using Natural Language.]]: (cite: journals/corr/abs-2106-02972/Goyal/2021) Proposes "Languaged Aided Reward and Value Adaptation". The paper originally set our to explore generalization after consuming a single video demonstration as part fo the model input, but also incorporated natural language descriptions as encoded by a sentence encoder. The model decomposes the task of predicting the goal state and predicting the reward/value of an observed state given the predicted goal state. The model architecture is simple - just a convolutional network combined with LSTM encodings of the sentence using FiLM. In the zero-shot setting the model was tuned by validating on one OOD candidate and testing on the other. Using language descriptions you can get decent OOD performance, but there is a caveat in the way that the model was tuned. [[papers/rl/grounded_language/generalization/zero_shot_task_adaptation_using_natural_language.md|Notes]]
### language_action_space
 - [[pdfs/rl/grounded_language/language_action_space/calm_language_models_action_generation.pdf|calm_language_models_action_generation]]: . [[papers/rl/grounded_language/language_action_space/calm_language_models_action_generation.md|Notes]]
 - [[pdfs/rl/grounded_language/language_action_space/kg_a2c_graph_constrained_action_spaces.pdf|kg_a2c_graph_constrained_action_spaces]]: . [[papers/rl/grounded_language/language_action_space/kg_a2c_graph_constrained_action_spaces.md|Notes]]
## hrl
 - [[pdfs/rl/hrl/attaining_interpretability_hierarchical_primitive_composition.pdf|attaining_interpretability_hierarchical_primitive_composition]]: . [[papers/rl/hrl/attaining_interpretability_hierarchical_primitive_composition.md|Notes]]
 - [[pdfs/rl/hrl/maxq.pdf|maxq]]: . [[papers/rl/hrl/maxq.md|Notes]]
 - [[pdfs/rl/hrl/precup_efficiency_bounds_hrl.pdf|precup_efficiency_bounds_hrl]]: . [[papers/rl/hrl/precup_efficiency_bounds_hrl.md|Notes]]
 - [[pdfs/rl/hrl/sutton_hrl.pdf|sutton_hrl]]: . [[papers/rl/hrl/sutton_hrl.md|Notes]]
 - [[pdfs/rl/hrl/the_promise_of_hrl_survey.pdf|the_promise_of_hrl_survey]]: . [[papers/rl/hrl/the_promise_of_hrl_survey.md|Notes]]
### goal_discovery
 - [[pdfs/rl/hrl/goal_discovery/adjacency_constrained_subgoals.pdf|adjacency_constrained_subgoals]]: . [[papers/rl/hrl/goal_discovery/adjacency_constrained_subgoals.md|Notes]]
 - [[pdfs/rl/hrl/goal_discovery/selfplay_goal_embeddings.pdf|selfplay_goal_embeddings]]: . [[papers/rl/hrl/goal_discovery/selfplay_goal_embeddings.md|Notes]]
### graphs
 - [[pdfs/rl/hrl/graphs/world_graphs_hrl.pdf|world_graphs_hrl]]: . [[papers/rl/hrl/graphs/world_graphs_hrl.md|Notes]]
### hindsight
 - [[pdfs/rl/hrl/hindsight/hide_functionally_decomposed_hierarchy.pdf|Learning Functionally Decomposed Hierarchies for Continuous Control Tasks With Path Planning.]]: (cite: journals/ral/ChristenJAH21) Functional decomposition of components. Planning and reward modelling are separate concerns. HiDE also proposes to use MVProp for planning. Reaching a sub-goal and planning sub-goals are done by different components of the system. [[papers/rl/hrl/hindsight/hide_functionally_decomposed_hierarchy.md|Notes]]
 - [[pdfs/rl/hrl/hindsight/representation_learning_hrl.pdf|Near-Optimal Representation Learning for Hierarchical Reinforcement Learning.]]: (cite: conf/iclr/NachumGLL19) Develops the notion of what a "sub-optimal" representation of a goal state is, defined in terms of an expected reward of the optimal hiearchical policy using that representation. Suboptimality defined as $\sup_{s \ in S} V^{\pi^*}(s) - V^{\pi^*_{\text{hier}}}(s)$, eg the worst case value function drop when using the optimal hierarchical policy compared to a normal optimal policy. This implies that you should set $k$-step goals such that a $k$-step goal reaching policy's state distribution energy function parameterized by the distance t othe goal is the *close to* the next-state distribution for the optimal policy. The low-level policy's reward is basically then the distance to the predicted goal plus the log-probability of its next-state transition. The experiments show that in an ant-navigation environment, the proposed method learns to represent the state as something close to x,y coordinates and predict the goals in terms of those x-y coordinates  [[papers/rl/hrl/hindsight/representation_learning_hrl.md|Notes]]
 - [[pdfs/rl/hrl/hindsight/hac_hierarchical_actor_critic.pdf|hac_hierarchical_actor_critic]]: . [[papers/rl/hrl/hindsight/hac_hierarchical_actor_critic.md|Notes]]
 - [[pdfs/rl/hrl/hindsight/hiro_data_efficient_hrl.pdf|hiro_data_efficient_hrl]]: . [[papers/rl/hrl/hindsight/hiro_data_efficient_hrl.md|Notes]]
 - [[pdfs/rl/hrl/hindsight/ho2_hindsight_off_policy.pdf|ho2_hindsight_off_policy]]: . [[papers/rl/hrl/hindsight/ho2_hindsight_off_policy.md|Notes]]
 - [[pdfs/rl/hrl/hindsight/revisiting_experience_replay_non_stationary_environments.pdf|revisiting_experience_replay_non_stationary_environments]]: . [[papers/rl/hrl/hindsight/revisiting_experience_replay_non_stationary_environments.md|Notes]]
### imitation
 - [[pdfs/rl/hrl/imitation/hierarchical_task_models_demonstration.pdf|hierarchical_task_models_demonstration]]: . [[papers/rl/hrl/imitation/hierarchical_task_models_demonstration.md|Notes]]
 - [[pdfs/rl/hrl/imitation/jing_adversarial_option_aware_hierarchical_imitation_learning.pdf|jing_adversarial_option_aware_hierarchical_imitation_learning]]: . [[papers/rl/hrl/imitation/jing_adversarial_option_aware_hierarchical_imitation_learning.md|Notes]]
### options
 - [[pdfs/rl/hrl/options/precup_attention_option_critic.pdf|precup_attention_option_critic]]: . [[papers/rl/hrl/options/precup_attention_option_critic.md|Notes]]
 - [[pdfs/rl/hrl/options/precup_option_critic.pdf|precup_option_critic]]: . [[papers/rl/hrl/options/precup_option_critic.md|Notes]]
 - [[pdfs/rl/hrl/options/precup_option_keyboard.pdf|precup_option_keyboard]]: . [[papers/rl/hrl/options/precup_option_keyboard.md|Notes]]
 - [[pdfs/rl/hrl/options/precup_options_deliberation_cost.pdf|precup_options_deliberation_cost]]: . [[papers/rl/hrl/options/precup_options_deliberation_cost.md|Notes]]
 - [[pdfs/rl/hrl/options/precup_termination_critic.pdf|precup_termination_critic]]: . [[papers/rl/hrl/options/precup_termination_critic.md|Notes]]
### probabilistic
 - [[pdfs/rl/hrl/probabilistic/stochastic_skills.pdf|stochastic_skills]]: . [[papers/rl/hrl/probabilistic/stochastic_skills.md|Notes]]
### representations
 - [[pdfs/rl/hrl/representations/language_as_abstraction_hrl.pdf|Language as an Abstraction for Hierarchical Deep Reinforcement Learning.]]: (cite: conf/nips/JiangGMF19) Proposes to use language as the abstraction for hierarchical reinforcement learning, since it provides unique compositional structure and could enable fast learning and combinatorial generalization. Learns an instruction-following low-level policy and a high-level policy that can re-use linguistic abstractions across tasks. Uses hindsight along with a re-labelling engine to re-label end-states with the correct goal so as to enable hindsight learning of the low-level policy. Similar to HER in that instead of re-labelling states as goals, you re-label them with the appropriate language. The high level policy produces language statements, the low-level policy consumes those language statements in order to determine its actions. This approach was tested in an environment where the tasks are more abstract, for example, objects should be sorted by size (so the high level policy creates instructions which tell the low-level policy how the objects should be re-ordered). The paper also studied compositional generalization, for example, in the training set "red" appears only in the first half of the instruction and in the test set "red" appears only in the second half.  The paper also propses to learn a "disentanged representaiton of language" using an InfoGAN and vector quantization. [[papers/rl/hrl/representations/language_as_abstraction_hrl.md|Notes]]
 - [[pdfs/rl/hrl/representations/sectar_hrl_trajectory_autoencoder.pdf|Self-Consistent Trajectory Autoencoder - Hierarchical Reinforcement Learning with Trajectory Embeddings.]]: (cite: conf/icml/Co-ReyesLGEAL18) . [[papers/rl/hrl/representations/sectar_hrl_trajectory_autoencoder.md|Notes]]
 - [[pdfs/rl/hrl/representations/structured_hierarchical_grammar.pdf|structured_hierarchical_grammar]]: . [[papers/rl/hrl/representations/structured_hierarchical_grammar.md|Notes]]
### search
 - [[pdfs/rl/hrl/search/subgoal_search_complex_reasoning.pdf|subgoal_search_complex_reasoning]]: . [[papers/rl/hrl/search/subgoal_search_complex_reasoning.md|Notes]]
### skills
 - [[pdfs/rl/hrl/skills/bagaria_deep_skill_graphs.pdf|bagaria_deep_skill_graphs]]: . [[papers/rl/hrl/skills/bagaria_deep_skill_graphs.md|Notes]]
 - [[pdfs/rl/hrl/skills/deep_skill_chaining_option_discovery.pdf|deep_skill_chaining_option_discovery]]: . [[papers/rl/hrl/skills/deep_skill_chaining_option_discovery.md|Notes]]
 - [[pdfs/rl/hrl/skills/hippo_sub_policy_adaptation_hrl.pdf|hippo_sub_policy_adaptation_hrl]]: . [[papers/rl/hrl/skills/hippo_sub_policy_adaptation_hrl.md|Notes]]
 - [[pdfs/rl/hrl/skills/latent_skill_planning_for_exploration_and_transfer.pdf|latent_skill_planning_for_exploration_and_transfer]]: . [[papers/rl/hrl/skills/latent_skill_planning_for_exploration_and_transfer.md|Notes]]
 - [[pdfs/rl/hrl/skills/learning_to_coordinate_skills.pdf|learning_to_coordinate_skills]]: . [[papers/rl/hrl/skills/learning_to_coordinate_skills.md|Notes]]
 - [[pdfs/rl/hrl/skills/skill_behaviour_diversification.pdf|skill_behaviour_diversification]]: . [[papers/rl/hrl/skills/skill_behaviour_diversification.md|Notes]]
 - [[pdfs/rl/hrl/skills/spirl_accelerated_rl_learned_skill_priors.pdf|spirl_accelerated_rl_learned_skill_priors]]: . [[papers/rl/hrl/skills/spirl_accelerated_rl_learned_skill_priors.md|Notes]]
 - [[pdfs/rl/hrl/skills/stochastic_skill_learning_hrl.pdf|stochastic_skill_learning_hrl]]: . [[papers/rl/hrl/skills/stochastic_skill_learning_hrl.md|Notes]]
### top_down
 - [[pdfs/rl/hrl/top_down/feudal_networks.pdf|feudal_networks]]: . [[papers/rl/hrl/top_down/feudal_networks.md|Notes]]
 - [[pdfs/rl/hrl/top_down/latent_space_hrl.pdf|latent_space_hrl]]: . [[papers/rl/hrl/top_down/latent_space_hrl.md|Notes]]
## imitation
 - [[pdfs/rl/imitation/adverarial_imitation_learning.pdf|Adversarial Soft Advantage Fitting - Imitation Learning without Policy Optimization.]]: (cite: conf/nips/BardeRJPPN20) Proposes to remove policy optimization from the adversarial imitation learning process by instead leveraging a novel discriminator formulation. The discriminator directly learns the optimal generator policy and therefore solves the generator's optimization problem "for free". This removes the reinforcement learning phase alltogether.  Effectively the optimal discriminator parameter is the "expert distribution" which in this case is the policy that we would have used RL To solve for. This policy matches the expert's trajectory distribution. Adversarial Soft Advantage Fitting alternates between two steps - first trinaing the discriminator by minimizing hte binary cross entropy loss between the discriminator's output on trajectories that came from the generator and trajectories that came from the expert, then updating the generator to maximize the discriminator loss. [[papers/rl/imitation/adverarial_imitation_learning.md|Notes]]
 - [[pdfs/rl/imitation/critic_regularized_regression.pdf|Critic Regularized Regression.]]: (cite: conf/nips/0001NZMSRSSGHF20) Offline reinforcement learning method to try and deal with overly-optimistic Q-estimates and inappropriate extrapolation beyond the observed data. CRR reduces offline policy optimization to a form of value-filtered regression which requires minimal algorithmic changes to standard actor-critic method. The main gist is that where trajectories are sub-optimal, only update the policy from the data where the Q-estimate indicates that the action taken in the data is better than the action that would have been taken by the policy. This means that you do not use the computed advantages when training the policy at all - just optimize the log-likelihood of actions which have a better Q-estimate than what the current policy would have chosen . [[papers/rl/imitation/critic_regularized_regression.md|Notes]]
## inverse_rl
 - [[pdfs/rl/inverse_rl/guided_cost_learning.pdf|Guided Cost Learning - Deep Inverse Optimal Control via Policy Optimization.]]: (cite: conf/icml/FinnLA16) Extends the MaxEnt method by addressing two challenges, the need for informative features and effective regularization and the difficulty of learning the cost function under unknown dynamics. To address the first problem, learn arbitrary nonlinear cost functions with a neural network. To deal with the second problem, formulate a sample-based approximation for MaxEnt. In Guided Cost Learning, you match the maximum entropy cost distribution (where the cost is the energy function) by optimizing a trajectory distribution with respect to the current cost using RL. Use the samples to both improve the policy and estimate the partition function. This basically requires both offline data and also a few online trajectories where you don't know the reward function but you optimize the online policy according to what your current estimate of the cost function is. . [[papers/rl/inverse_rl/guided_cost_learning.md|Notes]]
## learning_algorithms
 - [[pdfs/rl/learning_algorithms/ppo.pdf|Proximal Policy Optimization Algorithms.]]: (cite: journals/corr/SchulmanWDRK17/Schulman/2017) . [[papers/rl/learning_algorithms/ppo.md|Notes]]
 - [[pdfs/rl/learning_algorithms/action_dependent_baselines.pdf|action_dependent_baselines]]: . [[papers/rl/learning_algorithms/action_dependent_baselines.md|Notes]]
 - [[pdfs/rl/learning_algorithms/ddpg.pdf|ddpg]]: . [[papers/rl/learning_algorithms/ddpg.md|Notes]]
 - [[pdfs/rl/learning_algorithms/equivalence_policy_gradient_soft_q_learning.pdf|equivalence_policy_gradient_soft_q_learning]]: . [[papers/rl/learning_algorithms/equivalence_policy_gradient_soft_q_learning.md|Notes]]
 - [[pdfs/rl/learning_algorithms/gae_generalized_advantage_estimation.pdf|gae_generalized_advantage_estimation]]: . [[papers/rl/learning_algorithms/gae_generalized_advantage_estimation.md|Notes]]
 - [[pdfs/rl/learning_algorithms/phasic_policy_gradient.pdf|phasic_policy_gradient]]: . [[papers/rl/learning_algorithms/phasic_policy_gradient.md|Notes]]
 - [[pdfs/rl/learning_algorithms/soft_actor_critic.pdf|soft_actor_critic]]: . [[papers/rl/learning_algorithms/soft_actor_critic.md|Notes]]
## mdp
 - [[pdfs/rl/mdp/partially_observable_stochastic_games.pdf|partially_observable_stochastic_games]]: . [[papers/rl/mdp/partially_observable_stochastic_games.md|Notes]]
 - [[pdfs/rl/mdp/plannable_approximations_to_mdp_homomorphisms_equivariance_under_actions.pdf|plannable_approximations_to_mdp_homomorphisms_equivariance_under_actions]]: . [[papers/rl/mdp/plannable_approximations_to_mdp_homomorphisms_equivariance_under_actions.md|Notes]]
## memory
 - [[pdfs/rl/memory/memory_augmented_control.pdf|memory_augmented_control]]: . [[papers/rl/memory/memory_augmented_control.md|Notes]]
 - [[pdfs/rl/memory/working_memory_graphs.pdf|working_memory_graphs]]: . [[papers/rl/memory/working_memory_graphs.md|Notes]]
## multiagent
 - [[pdfs/rl/multiagent/learning_to_communicate_dial_rial.pdf|learning_to_communicate_dial_rial]]: . [[papers/rl/multiagent/learning_to_communicate_dial_rial.md|Notes]]
 - [[pdfs/rl/multiagent/maddpg.pdf|maddpg]]: . [[papers/rl/multiagent/maddpg.md|Notes]]
## multitask
 - [[pdfs/rl/multitask/mtan_end_to_end_multi_task_learning_with_attention.pdf|Multi-Task Learning With Attention for End-to-End Autonomous Driving.]]: (cite: conf/cvpr/IshiharaKMH21) Task specific channel attention - shared backbone with different channel attention networks per task, which perform a form of feature selection like FiLM (MTAN). [[papers/rl/multitask/mtan_end_to_end_multi_task_learning_with_attention.md|Notes]]
 - [[pdfs/rl/multitask/softmodule_multi_task_rl_with_soft_modularization.pdf|Multi-Task Reinforcement Learning with Soft Modularization.]]: (cite: conf/nips/YangXWW20) Avoid gradient conflicts by using different modules for different tasks and then learning to route inputs between them (SoftModule). [[papers/rl/multitask/softmodule_multi_task_rl_with_soft_modularization.md|Notes]]
 - [[pdfs/rl/multitask/distributional_view_on_multi_objective_policy_optimization.pdf|distributional_view_on_multi_objective_policy_optimization]]: . [[papers/rl/multitask/distributional_view_on_multi_objective_policy_optimization.md|Notes]]
 - [[pdfs/rl/multitask/multi_objective_distributional.pdf|multi_objective_distributional]]: . [[papers/rl/multitask/multi_objective_distributional.md|Notes]]
 - [[pdfs/rl/multitask/multitask_policy_sketches.pdf|multitask_policy_sketches]]: . [[papers/rl/multitask/multitask_policy_sketches.md|Notes]]
## offline
 - [[pdfs/rl/offline/conservative_q_learning_offline_rl.pdf|Conservative Q-Learning for Offline Reinforcement Learning.]]: (cite: conf/nips/KumarZTL20) Tries to tackle the problem of Q-function over-estimation for unseen actions when learning offline (since there are rarely any "failed" examples). Limits the learning of the Q function such that the expected value of a policy under the Q function is a lower bound on its true value. This can be used as a regularizer. In essence, what it does is that it tries to minimize the Q function values under a hypothetical policy that tries to exploit the Q function in order to get a maximal Q value. [[papers/rl/offline/conservative_q_learning_offline_rl.md|Notes]]
 - [[pdfs/rl/offline/valuedice_discriminator_actor_critic_offline_imitation_learning.pdf|Discriminator-Actor-Critic - Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning.]]: (cite: conf/iclr/KostrikovADLT19) Tries to address the implicit bias present in the reward functions in the adversarial imitation learning framework as well as the number of interactions required with the requirement in order to properly learn to imitate the expert's policy. Proposes a new algorithm called "Discriminator Actor-Critic" that uses off-policy RL to reduce policy-environment interaction sample complexity. To address the sample inefficiency problem, sample transitions from a replay buffer while performing off-polciy training for the generator. In order to recover the originak on-policy expectation, use importance sampling when evaluating the discriminator on the replay buffer  . [[papers/rl/offline/valuedice_discriminator_actor_critic_offline_imitation_learning.md|Notes]]
 - [[pdfs/rl/offline/gail_generative_adversarial_imitation_learning.pdf|Generative Adversarial Imitation Learning.]]: (cite: journals/corr/HoE16) Train a generative distribution for state-visitation that approximates the data distribution. The discriminator must classify if the state-action pairs come from the agent or expert and the generator's job is to fool the discriminator similar to GAN. [[papers/rl/offline/gail_generative_adversarial_imitation_learning.md|Notes]]
 - [[pdfs/rl/offline/robust_rewards_with_adversarial_inverse_rl.pdf|Learning Robust Rewards with Adversarial Inverse Reinforcement Learning.]]: (cite: journals/corr/abs-1710-11248/Fu/2017) Propose AIRL, a practical and scaleable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. AIRL is capable of recovering reward functions robust to changes in dynamics. Compared to GAIL, GAIL doesn't attempt to recover a reward function, it just attempts to recover a discriminator for trajectories and therefore the policy for the expert via the generator. This work builds on GAIL by using a discriminator that corresponds to an odds-ratio between the policy and exponentiated reward distribution. In the single state and action case you have $D_{\theta}(s, a) = \frac{\exp{f_{\theta}(s, a)}}{\exp{f_{\theta}(s, a)} + \pi(a|s)}$ and at optimality, $f$ equasl the log-policy.  The problem is reward ambiguiuty. The paper goes on to formulate what a "disentangled reward function" is (eg, one where the optimal policy is the same no matter what the dynamics).  To remove unwanted reward shaping with arbitrary reward function classes, the learned reward can only depend on $s$.  The paper proposes to instead limit $f$ to be a composition of $g$ and $h$, where $h$ is the reward shaping term. [[papers/rl/offline/robust_rewards_with_adversarial_inverse_rl.md|Notes]]
 - [[pdfs/rl/offline/gscl_learning_to_reach_goals_via_iterated_supervised_learning.pdf|Learning to Reach Goals via Iterated Supervised Learning.]]: (cite: conf/iclr/Ghosh0RFDEL21) Study RL algorithms that use imitation learning to acquire goal-reaching policies from scratch without the need for expert demonstrations or a value function. Leverage the idea that any trajectory is a successful demonstration of reaching its final state. Propose a simple algorithm where the agent continually relabels and imitates trajectories that it generates to progressively learn new goal-reaching behaviours from scratch. Essentially, collect policy rollouts, relabel the goals, then do behavioural cloning on those relabelled trajectories. There is no need to maintain an estimate of the value function. Authors then tested this method in a goal-conditioned setup. Starting from scratch, GSCL can beat TD3-HER - the inputs to the policy are the goal and the states and at the end of each episode you either relabel the trajectory and give a reward of zero, or give a reward of 1 and don't relabel. When comparing with PPO, there is no goal-relabelling, so instead a surrogate $\epsilon$-ball indicator reward function is given and the hindsight methods can still beat this. [[papers/rl/offline/gscl_learning_to_reach_goals_via_iterated_supervised_learning.md|Notes]]
 - [[pdfs/rl/offline/offline_reinforcement_learning_tutorial_levine.pdf|Offline Reinforcement Learning - Tutorial, Review, and Perspectives on Open Problems.]]: (cite: journals/corr/abs-2005-01643/Levine/2020) Discusses some applications of offline reinforcement learning, which includes "learning goal-directed dialogue policies", since collecting trials requires interacting with a live human, which could be very expensive. Summarized what makes offline reinforcement learning difficult, namely that you cannot explore the environment, so one assumption that you have to amke is that the dataset covers high-reward transitions. But another challenge is that learning a policy that does something different from the observed dataset is difficult because its out of distribution. There are theoretical bounds on the error that come from Behavioural Cloning and DAgger, which are basically that we get an error bound that is at best quadratic in the time horizon H in the offline case, but linear in H in the online case. Also notes that in the offline case, MBRL can suffer from poor performance as a result of distribution shift.  [[papers/rl/offline/offline_reinforcement_learning_tutorial_levine.md|Notes]]
 - [[pdfs/rl/offline/rce_replacing_rewards_with_examples.pdf|Replacing Rewards with Examples - Example-Based Policy Search via Recursive Classification.]]: (cite: journals/corr/abs-2103-12656/Eysenbach/2021) Defining reward functions can be difficult to do in practice - can we instead use examples in order to derive a reward? This is like a form of inverse reinforcement learning, where just have a bunch of labelled goal states. Define a classifier which estimates whether we will be successful given a current state-action pair and bootstrap the trajectory probabilities. [[papers/rl/offline/rce_replacing_rewards_with_examples.md|Notes]]
 - [[pdfs/rl/offline/sqil_offline_imitation_learning_via_rl_sparse_rewards.pdf|SQIL - Imitation Learning via Reinforcement Learning with Sparse Rewards.]]: (cite: conf/iclr/ReddyDL20) Provide an incentive to the agent to match the demonstrations over a long horizon by encouraging it to return to demonstrated states upon encountering new OOD states. This is called "Soft Q Imitation Learning (SQIL)". This is a regularized variant of behavioural cloning which uses a sparsity prior to encourage long-horizon imitation. The algorithm gives a reward of 1 for all samples in the dataset and reward of zero for everything sampled online. In that sense, visiting states outside of the data manifold get low Q values due to the reward of zero, but going from an unknown-state back to a known state will get a positive reward (due to adding the log-sum-exponents of the next-state, next-action pairs). There is more detail provided in equation 1 of the paper.  [[papers/rl/offline/sqil_offline_imitation_learning_via_rl_sparse_rewards.md|Notes]]
 - [[pdfs/rl/offline/deepaveragers_offline_rl_by_solving_derived_nonparametric_mdps.pdf|deepaveragers_offline_rl_by_solving_derived_nonparametric_mdps]]: . [[papers/rl/offline/deepaveragers_offline_rl_by_solving_derived_nonparametric_mdps.md|Notes]]
 - [[pdfs/rl/offline/lompo-offline-rl-images-latent-space.pdf|lompo-offline-rl-images-latent-space]]: . [[papers/rl/offline/lompo-offline-rl-images-latent-space.md|Notes]]
 - [[pdfs/rl/offline/workflow_offline_model_free_robotic_rl.pdf|workflow_offline_model_free_robotic_rl]]: . [[papers/rl/offline/workflow_offline_model_free_robotic_rl.md|Notes]]
## planning
 - [[pdfs/rl/planning/consciousness_inspired_planning_mbrl_set_representation.pdf|A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning.]]: (cite: journals/corr/abs-2106-02097/Zhao/2021) Combines a model-based deep reinforcement learning agent with a sparsity inducing bottleneck in its attention to the state. This forces the number of entities to which the agent attends to be small. The query is a set of inducing points and a vector for the action under consideration and the keys come from the object. Hard-attention is performed, so only the top $k$ matching values are kept. Tested on out-of-distribution environment layouts. [[papers/rl/planning/consciousness_inspired_planning_mbrl_set_representation.md|Notes]]
 - [[pdfs/rl/planning/compositional_rl_logical_specifications.pdf|Compositional Reinforcement Learning from Logical Specifications.]]: (cite: journals/corr/abs-2106-13906/Jothimurugan/2021) Study the problem of learning control policies for complex tasks given by logical specifications. This work develops a compositional learning approach called DIRL. First encode the specification into an abstract graph, then incorporate reinforcement learning to learn a policy for each edge within a Dijkstra-style planning algorithm to compute a high-level plan. In this case the high-level specifications are given by a formal language already. So to combine the two, the only learning that you need to do on the "high level" is estimate the probability that a policy for a given edge safely takes you from one node to the next. Then you can allocate the negative log probability as the "cost" for that edge, meaning that transitions that are less likely to be successful have a higher cost. The higher level planner which fuses all the policies together is just Djikstra's algorithm for finding a shortest path to complete the specification. [[papers/rl/planning/compositional_rl_logical_specifications.md|Notes]]
 - [[pdfs/rl/planning/dreamer_dream_to_control.pdf|Dream to Control - Learning Behaviors by Latent Imagination.]]: (cite: conf/iclr/HafnerLB020) . [[papers/rl/planning/dreamer_dream_to_control.md|Notes]]
 - [[pdfs/rl/planning/dreamerv2.pdf|Mastering Atari with Discrete World Models.]]: (cite: conf/iclr/HafnerL0B21) Presents the DreamerV2 architecture. [[papers/rl/planning/dreamerv2.md|Notes]]
 - [[pdfs/rl/planning/2020_mbrl_survey.pdf|Model-based Reinforcement Learning - A Survey.]]: (cite: journals/corr/abs-2006-16712/Moerland/2020) . [[papers/rl/planning/2020_mbrl_survey.md|Notes]]
 - [[pdfs/rl/planning/chane-sane_goal_conditioned_rl_imagined_subgoals.pdf|chane-sane_goal_conditioned_rl_imagined_subgoals]]: . [[papers/rl/planning/chane-sane_goal_conditioned_rl_imagined_subgoals.md|Notes]]
 - [[pdfs/rl/planning/decision_transformer.pdf|decision_transformer]]: . [[papers/rl/planning/decision_transformer.md|Notes]]
 - [[pdfs/rl/planning/gnn_motion_planning.pdf|gnn_motion_planning]]: . [[papers/rl/planning/gnn_motion_planning.md|Notes]]
 - [[pdfs/rl/planning/grounding_complex_navigational_instructions_using_scene_graphs.pdf|grounding_complex_navigational_instructions_using_scene_graphs]]: . [[papers/rl/planning/grounding_complex_navigational_instructions_using_scene_graphs.md|Notes]]
 - [[pdfs/rl/planning/learning_what_if_explanations_for_sequential_decision_making.pdf|learning_what_if_explanations_for_sequential_decision_making]]: . [[papers/rl/planning/learning_what_if_explanations_for_sequential_decision_making.md|Notes]]
 - [[pdfs/rl/planning/local_experience_global_planning.pdf|local_experience_global_planning]]: . [[papers/rl/planning/local_experience_global_planning.md|Notes]]
 - [[pdfs/rl/planning/mbold_model_based_visual_planning_with_self_supervised_functional_distances.pdf|mbold_model_based_visual_planning_with_self_supervised_functional_distances]]: . [[papers/rl/planning/mbold_model_based_visual_planning_with_self_supervised_functional_distances.md|Notes]]
 - [[pdfs/rl/planning/motion_planning_networks.pdf|motion_planning_networks]]: . [[papers/rl/planning/motion_planning_networks.md|Notes]]
 - [[pdfs/rl/planning/muzero.pdf|muzero]]: . [[papers/rl/planning/muzero.md|Notes]]
 - [[pdfs/rl/planning/on_the_role_of_planning_in_model_based_deep_reinforcement_learning.pdf|on_the_role_of_planning_in_model_based_deep_reinforcement_learning]]: . [[papers/rl/planning/on_the_role_of_planning_in_model_based_deep_reinforcement_learning.md|Notes]]
 - [[pdfs/rl/planning/oracle_planning_clash_of_clans.pdf|oracle_planning_clash_of_clans]]: . [[papers/rl/planning/oracle_planning_clash_of_clans.md|Notes]]
 - [[pdfs/rl/planning/ozair_vector_quantized_models_for_planning.pdf|ozair_vector_quantized_models_for_planning]]: . [[papers/rl/planning/ozair_vector_quantized_models_for_planning.md|Notes]]
 - [[pdfs/rl/planning/plan_arithmetic.pdf|plan_arithmetic]]: . [[papers/rl/planning/plan_arithmetic.md|Notes]]
 - [[pdfs/rl/planning/plan_based_relaxed_reward_shaping_for_goal_directed_tasks.pdf|plan_based_relaxed_reward_shaping_for_goal_directed_tasks]]: . [[papers/rl/planning/plan_based_relaxed_reward_shaping_for_goal_directed_tasks.md|Notes]]
 - [[pdfs/rl/planning/planning_from_pixels_inv_dynamics.pdf|planning_from_pixels_inv_dynamics]]: . [[papers/rl/planning/planning_from_pixels_inv_dynamics.md|Notes]]
 - [[pdfs/rl/planning/planning_from_pixels_latent_dynamics.pdf|planning_from_pixels_latent_dynamics]]: . [[papers/rl/planning/planning_from_pixels_latent_dynamics.md|Notes]]
 - [[pdfs/rl/planning/portable_rep_high_level_planning.pdf|portable_rep_high_level_planning]]: . [[papers/rl/planning/portable_rep_high_level_planning.md|Notes]]
 - [[pdfs/rl/planning/sample_efficient_cem_planning.pdf|sample_efficient_cem_planning]]: . [[papers/rl/planning/sample_efficient_cem_planning.md|Notes]]
 - [[pdfs/rl/planning/vector_quantized_models_for_planning.pdf|vector_quantized_models_for_planning]]: . [[papers/rl/planning/vector_quantized_models_for_planning.md|Notes]]
 - [[pdfs/rl/planning/visual_planning_skip_connections.pdf|visual_planning_skip_connections]]: . [[papers/rl/planning/visual_planning_skip_connections.md|Notes]]
 - [[pdfs/rl/planning/vtnet_visual_transformer_network_for_object_goal_navigation.pdf|vtnet_visual_transformer_network_for_object_goal_navigation]]: . [[papers/rl/planning/vtnet_visual_transformer_network_for_object_goal_navigation.md|Notes]]
### disentanglement
 - [[pdfs/rl/planning/disentanglement/composable_planning_with_attributes_disentanglement.pdf|Composable Planning with Attributes.]]: (cite: conf/icml/ZhangSLSF18) If the agent knows which properties of the environment are important, then after learning how its actions affect those properties, it may be able to use this knowledge to solve complex tasks without specifically training for them. This paper considers a setup where the environment is augmented with a set of user-defined attributes that parameterize features of interest and proposes to learn policies for transitioning between attributes and maintaining a graph of possible transitions. Effectively, the policy takes a pair of inputs, the current state and the attributes of a goal state and outputs a distribution over actions. This is similar to [[compositional_rl_logical_specifications]] in a way, since both methods also use a transition table, but in this case the attributes are provided by the neural network. [[papers/rl/planning/disentanglement/composable_planning_with_attributes_disentanglement.md|Notes]]
### generalization
 - [[pdfs/rl/planning/generalization/procedural_generalization_by_planning_self_supervised_world_models.pdf|Procedural Generalization by Planning with Self-Supervised World Models.]]: (cite: journals/corr/abs-2111-01587/Anand/2021) Investigates generalization capability of planning, and representation learning via MuZero. Results indicated that observation generalization was possible when planning decoupled from representation learning, but task generalization did not always work. [[papers/rl/planning/generalization/procedural_generalization_by_planning_self_supervised_world_models.md|Notes]]
 - [[pdfs/rl/planning/generalization/trajectory_wise_multiple_choice_learning_for_dynamics_regularization.pdf|Trajectory-wise Multiple Choice Learning for Dynamics Generalization in Reinforcement Learning.]]: (cite: conf/nips/SeoLGKSA20) . [[papers/rl/planning/generalization/trajectory_wise_multiple_choice_learning_for_dynamics_regularization.md|Notes]]
### probabilistic
 - [[pdfs/rl/planning/probabilistic/continuous_time_mbrl.pdf|continuous_time_mbrl]]: . [[papers/rl/planning/probabilistic/continuous_time_mbrl.md|Notes]]
 - [[pdfs/rl/planning/probabilistic/sample_efficient_rl_using_deep_gps.pdf|sample_efficient_rl_using_deep_gps]]: . [[papers/rl/planning/probabilistic/sample_efficient_rl_using_deep_gps.md|Notes]]
### value
 - [[pdfs/rl/planning/value/chaplot_spatial_planning_transformers.pdf|Differentiable Spatial Planning using Transformers.]]: Use a big pre-trained transformer to perform a similar task that VINs perform. The claim is that planning problems have statistical regularities to them - usually maps to plan on are not noise. SPT can predict multiple steps ahead in one go and deal with the case where the map is unknown and is predicted as a function of the observations . [[papers/rl/planning/value/chaplot_spatial_planning_transformers.md|Notes]]
 - [[pdfs/rl/planning/value/lutter_value_iteration_continuous_actions_states_and_time.pdf|Value Iteration in Continuous Actions, States and Time.]]: Provides a method to perform value iteration when the state space is continuous. Proposes "Continuous Fitted Value Iteration". If you have a known dynamics model, then the optimal policy and value function can be derived for non-linear control affine dynamics. [[papers/rl/planning/value/lutter_value_iteration_continuous_actions_states_and_time.md|Notes]]
 - [[pdfs/rl/planning/value/gvin_life_beyond_lattices.pdf|gvin_life_beyond_lattices]]: . [[papers/rl/planning/value/gvin_life_beyond_lattices.md|Notes]]
 - [[pdfs/rl/planning/value/motion_planning_transformers.pdf|motion_planning_transformers]]: . [[papers/rl/planning/value/motion_planning_transformers.md|Notes]]
 - [[pdfs/rl/planning/value/mvprop.pdf|mvprop]]: . [[papers/rl/planning/value/mvprop.md|Notes]]
 - [[pdfs/rl/planning/value/predictron.pdf|predictron]]: . [[papers/rl/planning/value/predictron.md|Notes]]
 - [[pdfs/rl/planning/value/value_iteration_networks.pdf|value_iteration_networks]]: . [[papers/rl/planning/value/value_iteration_networks.md|Notes]]
 - [[pdfs/rl/planning/value/xlvin_executed_latent_vin.pdf|xlvin_executed_latent_vin]]: . [[papers/rl/planning/value/xlvin_executed_latent_vin.md|Notes]]
## relational
 - [[pdfs/rl/relational/deep_rl_relational_inductive_biases_lillicrap_zambaldi.pdf|Deep reinforcement learning with relational inductive biases.]]: Encode the image into feature maps using a convolutional encoder. Treat each pixel as an "entity", then create a fully connected graph of entities which update their states via attention-weighted message passing. (Note: This seems very similar to just taking a transformer to the pixels). Then pool the updated entity embeddings and use an MLP to produce a policy. Tested on a box-world environment where an agent must unlock boxes in sequence and showed that the model generalizes the longer sequence lengths and new key-lock combinations. Seems to require several hundred million environment steps to solve. [[papers/rl/relational/deep_rl_relational_inductive_biases_lillicrap_zambaldi.md|Notes]]
## representations
 - [[pdfs/rl/representations/contrastive_explanations_for_rl_self_predictions.pdf|contrastive_explanations_for_rl_self_predictions]]: . [[papers/rl/representations/contrastive_explanations_for_rl_self_predictions.md|Notes]]
 - [[pdfs/rl/representations/data_efficient_rl_self_predictive_representations.pdf|data_efficient_rl_self_predictive_representations]]: . [[papers/rl/representations/data_efficient_rl_self_predictive_representations.md|Notes]]
 - [[pdfs/rl/representations/farm_feature_attending_recurrent_modules_for_generalization_in_rl.pdf|farm_feature_attending_recurrent_modules_for_generalization_in_rl]]: . [[papers/rl/representations/farm_feature_attending_recurrent_modules_for_generalization_in_rl.md|Notes]]
 - [[pdfs/rl/representations/learning_from_demonstration_with_weakly_supervised_disentanglement.pdf|learning_from_demonstration_with_weakly_supervised_disentanglement]]: . [[papers/rl/representations/learning_from_demonstration_with_weakly_supervised_disentanglement.md|Notes]]
 - [[pdfs/rl/representations/reinforcement_learning_prototypical_representations.pdf|reinforcement_learning_prototypical_representations]]: . [[papers/rl/representations/reinforcement_learning_prototypical_representations.md|Notes]]
 - [[pdfs/rl/representations/representation_matters_improving_perception_and_exploration_higgins.pdf|representation_matters_improving_perception_and_exploration_higgins]]: . [[papers/rl/representations/representation_matters_improving_perception_and_exploration_higgins.md|Notes]]
 - [[pdfs/rl/representations/return_based_contrastive_repre.pdf|return_based_contrastive_repre]]: . [[papers/rl/representations/return_based_contrastive_repre.md|Notes]]
 - [[pdfs/rl/representations/self_supervised_visual_reinfor.pdf|self_supervised_visual_reinfor]]: . [[papers/rl/representations/self_supervised_visual_reinfor.md|Notes]]
 - [[pdfs/rl/representations/smorl_self_supervised_visual_reinforcement_learning_with_object_centric_representations.pdf|smorl_self_supervised_visual_reinforcement_learning_with_object_centric_representations]]: . [[papers/rl/representations/smorl_self_supervised_visual_reinforcement_learning_with_object_centric_representations.md|Notes]]
 - [[pdfs/rl/representations/stooke_decoupling_representation_learning_from_rl.pdf|stooke_decoupling_representation_learning_from_rl]]: . [[papers/rl/representations/stooke_decoupling_representation_learning_from_rl.md|Notes]]
## symmetry
 - [[pdfs/rl/symmetry/block_mdps_robust_state_abstractions.pdf|block_mdps_robust_state_abstractions]]: . [[papers/rl/symmetry/block_mdps_robust_state_abstractions.md|Notes]]
 - [[pdfs/rl/symmetry/mdp_homomorphic_networks.pdf|mdp_homomorphic_networks]]: . [[papers/rl/symmetry/mdp_homomorphic_networks.md|Notes]]
 - [[pdfs/rl/symmetry/symmetry_based_disentangled_representation_learning_interation.pdf|symmetry_based_disentangled_representation_learning_interation]]: . [[papers/rl/symmetry/symmetry_based_disentangled_representation_learning_interation.md|Notes]]
## transfer
 - [[pdfs/rl/transfer/modular_neural_network_policies_for_multi_task_and_multi_robot_transfer.pdf|Learning modular neural network policies for multi-task and multi-robot transfer.]]: Decompose into "task specific" and "robot specific" modules to enable zero-shot generalization with a variety of different robots. [[papers/rl/transfer/modular_neural_network_policies_for_multi_task_and_multi_robot_transfer.md|Notes]]
 - [[pdfs/rl/transfer/darla_zero_shot_transfer_reinforcement_learning_disentanglement.pdf|darla_zero_shot_transfer_reinforcement_learning_disentanglement]]: . [[papers/rl/transfer/darla_zero_shot_transfer_reinforcement_learning_disentanglement.md|Notes]]
 - [[pdfs/rl/transfer/disentangled_cumulants_successor_representations_transfer.pdf|disentangled_cumulants_successor_representations_transfer]]: . [[papers/rl/transfer/disentangled_cumulants_successor_representations_transfer.md|Notes]]
 - [[pdfs/rl/transfer/parrot_data_driven_behavioural_priors_rl.pdf|parrot_data_driven_behavioural_priors_rl]]: . [[papers/rl/transfer/parrot_data_driven_behavioural_priors_rl.md|Notes]]
## transformer
 - [[pdfs/rl/transformer/sensory_neuron_as_transformer_permutation_invariant_nn_rl.pdf|The Sensory Neuron as a Transformer - Permutation-Invariant Neural Networks for Reinforcement Learning.]]: Split image into patches and use a transformer without positional encodings. This is permutation invariant at test-time (to permutations of patches). [[papers/rl/transformer/sensory_neuron_as_transformer_permutation_invariant_nn_rl.md|Notes]]
 - [[pdfs/rl/transformer/transformers_are_meta_reinforcement_learners.pdf|Transformers are Meta-Reinforcement Learners]]: Treat the meta-reinforcement learning problem as a sequence learning problem where meta-training consists of feeding many training instances of a task into a transformer. [[papers/rl/transformer/transformers_are_meta_reinforcement_learners.md|Notes]]
 - [[pdfs/rl/transformer/trajectory-transformer-neurips-2021.pdf|trajectory-transformer-neurips-2021]]: . [[papers/rl/transformer/trajectory-transformer-neurips-2021.md|Notes]]
## world_models
 - [[pdfs/rl/world_models/augwm_augmented_world_models_facilitate_zero_shot_dynamics_generalization.pdf|Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment.]]: . [[papers/rl/world_models/augwm_augmented_world_models_facilitate_zero_shot_dynamics_generalization.md|Notes]]
 - [[pdfs/rl/world_models/contrastive_learning_structured_world_models.pdf|contrastive_learning_structured_world_models]]: . [[papers/rl/world_models/contrastive_learning_structured_world_models.md|Notes]]
 - [[pdfs/rl/world_models/contrastive_structured_world_models.pdf|contrastive_structured_world_models]]: . [[papers/rl/world_models/contrastive_structured_world_models.md|Notes]]
 - [[pdfs/rl/world_models/structured_world_belief_for_rl_pomdp.pdf|structured_world_belief_for_rl_pomdp]]: . [[papers/rl/world_models/structured_world_belief_for_rl_pomdp.md|Notes]]
# speech
 - [[pdfs/speech/vq_wave2vec.pdf|vq_wave2vec]]: . [[papers/speech/vq_wave2vec.md|Notes]]
# transfer
## mixed
 - [[pdfs/transfer/mixed/visual_nlp_supervision.pdf|visual_nlp_supervision]]: . [[papers/transfer/mixed/visual_nlp_supervision.md|Notes]]
