---
title: Ethical and social risks of harm from Language Models.
venue: CoRR
volume: abs/2112.04359
year: 2021
type: Informal Publications
access: open
key: journals/corr/abs-2112-04359
ee: https://arxiv.org/abs/2112.04359
url: https://dblp.org/rec/journals/corr/abs-2112-04359
authors: ["Laura Weidinger", "John Mellor", "Maribeth Rauh", "Conor Griffin", "Jonathan Uesato", "Po-Sen Huang", "Myra Cheng", "Mia Glaese", "Borja Balle", "Atoosa Kasirzadeh", "Zac Kenton", "Sasha Brown", "Will Hawkins", "Tom Stepleton", "Courtney Biles", "Abeba Birhane", "Julia Haas", "Laura Rimell", "Lisa Anne Hendricks", "William S. Isaac", "Sean Legassick", "Geoffrey Irving", "Iason Gabriel"]
sync_version: 3
cite_key: journals/corr/abs-2112-04359/Weidinger/2021
---

From Table 1, the main categories are:

1. Discrimination, Exclusion and Toxicity: Social stereotypes, exclusionary norms, toxic language, lower performance by social group
	1. stems from training data
	2. underrepresented groups
	3. historic patterns of systemtic injustice
	4. exclusionary norms: What if you try to deal with this by hardcoding? Problem: value lock-in, mitigating risk works against the functionality of the language model. Don't make default assumptions. Societal progress means that although a few hundred years ago that the genders were "he" and "she" and there are no gay people etc. If these are hardcoded using domain knowledge in the system, this means that if the society progresses the language model does not progress with it. Hardcoding requires a notion of objective truth.
2. Information Hazards: Compromising privacy by leaking information or inferring private information
3. Misinformation Harms: Disseminating false information
	1. eg "what side of the road do people drive on?" "Right", but in your case its left!
4. Malicious Uses: Malware, facilitating fraud / phishing, censorship, surveillance
	1. Scams, impersonation
5. HCI Harms: Harms from anthropomorphization
	1. Voice assistants are mostly female and submissive / or white "intelligent"
6. Automation harms: Automating out creative industry
7. Access harms: Cost required to operate and inability to access, inequality due to power differential
8. Feedback loop harms: Reinforcing stereotypes, learning from own bad predictions
9. Environmental harms: Operating costs

# Discussion

Could and should it be regulated which news sites language models get their news from?