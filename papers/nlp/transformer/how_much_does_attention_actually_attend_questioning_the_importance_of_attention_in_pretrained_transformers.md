---
title: "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers."
venue: "CoRR"
volume: "abs/2211.03495"
year: "2022"
type: "Informal Publications"
access: "open"
key: "journals/corr/abs-2211-03495"
doi: "10.48550/ARXIV.2211.03495"
ee: "https://doi.org/10.48550/arXiv.2211.03495"
url: "https://dblp.org/rec/journals/corr/abs-2211-03495"
authors: ["Michael Hassid", "Hao Peng", "Daniel Rotem", "Jungo Kasai", "Ivan Montero", "Noah A. Smith", "Roy Schwartz"]
sync_version: 3
cite_key: "journals/corr/abs-2211-03495/Hassid/2022"
---
