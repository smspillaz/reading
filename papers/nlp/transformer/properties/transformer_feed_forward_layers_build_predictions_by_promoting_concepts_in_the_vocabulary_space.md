---
title: "Transformer Feed-Forward Layers Are Key-Value Memories."
venue: "EMNLP"
pages: "5484-5495"
year: "2021"
type: "Conference and Workshop Papers"
access: "open"
key: "conf/emnlp/GevaSBL21"
doi: "10.18653/V1/2021.EMNLP-MAIN.446"
ee: "https://doi.org/10.18653/v1/2021.emnlp-main.446"
url: "https://dblp.org/rec/conf/emnlp/GevaSBL21"
authors: ["Mor Geva", "Roei Schuster", "Jonathan Berant", "Omer Levy"]
sync_version: 3
cite_key: "conf/emnlp/GevaSBL21"
---

In this paper, they reverse-engineer the operation of the FFN layers in Transformers. The output from each FFN is sort of like an additive update to a distribution over output tokens.