---
title: "Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks."
venue: "CoRR"
volume: "abs/2210.00400"
year: "2022"
type: "Informal Publications"
access: "open"
key: "journals/corr/abs-2210-00400"
doi: "10.48550/ARXIV.2210.00400"
ee: "https://doi.org/10.48550/arXiv.2210.00400"
url: "https://dblp.org/rec/journals/corr/abs-2210-00400"
authors: ["Yuxuan Li", "James L. McClelland"]
sync_version: 3
cite_key: "journals/corr/abs-2210-00400/Li/2022"
---
In this paper they explore how well a transformer can perform tasks like copying, sorting and hierarchical compositions.

Strong generalization happens to sequences longer than those used in training. You can get this by replacing the stardard positional encoding by labels arbitrarily paired with items in the sequence.