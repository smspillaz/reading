---
title: Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation.
venue: ICLR
year: 2021
type: Conference and Workshop Papers
access: open
key: conf/iclr/ParisottoS21
ee: https://openreview.net/forum?id=uR9LaO_QxF
url: https://dblp.org/rec/conf/iclr/ParisottoS21
authors: ["Emilio Parisotto", "Russ R. Salakhutdinov"]
sync_version: 3
cite_key: conf/iclr/ParisottoS21
---
In a nutshell - distill the transformer into an LSTM, so that you get the transformer's superior sample efficiency, while keeping the LSTM's computational efficiency.