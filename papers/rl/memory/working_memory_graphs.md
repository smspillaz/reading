---
title: Working Memory Graphs.
venue: ICML
pages: 6404-6414
year: 2020
type: Conference and Workshop Papers
access: open
key: conf/icml/LoyndFcSH20
ee: http://proceedings.mlr.press/v119/loynd20a.html
url: https://dblp.org/rec/conf/icml/LoyndFcSH20
authors: ["Ricky Loynd", "Roland Fernandez", "Asli Celikyilmaz", "Adith Swaminathan", "Matthew J. Hausknecht"]
sync_version: 3
cite_key: conf/icml/LoyndFcSH20
---
# Working Memory Graphs

Agent employs multi-head self-attention to reason over a dynamic set of vectors
representing observed and recurrent state. Significant gains in learning efficiency reported.

## Determining the state of the system

POMDP / DQN: Just condition over a concatenation of observations. Or encode them
using an LSTM.

## This work

Transformer based agend with many shortcut paths for information to
flow from past observations to current actions through memos.

Transformer was tried before in [[a_simple_neural_attentive_meta_learner]] but it didn't work so well.

### Working Memory Graphs

![[working_memory_graphs_architecture.png]]

![[wmg_comparison_to_rnn.png]]

General idea: Apply self-attention between your "memos" and "factored observations"
coming from the system.

On each timestep, replace the oldest memo with a new memo as generated by
a nonlinear function of the core's output vector and the model parameters.

Unlike other works, you update only *one* memoy on each timestep to maximize memo persistence and facilitate preservation of information through time.

Each memo vector defines one row in a memo matrix, $\mathbf{M} \in \mathbb{R}^{n_M \times d_M}$, $n_M$ being the number of memos and $d_M$ being the size of each memo vector. Memos are initialized to zero. Think of it as several different hidden states.

The update step looks like this:

$$
T^{\text{in}} = \begin{bmatrix} c W_{\text{core}} + b_{\text{core}} \\ F W_{\text{fac}} + b_{\text{fac}} \\ M' W_{\text{mem}} + b_{\text{mem}} \end{bmatrix}
$$

$$
M = \begin{bmatrix} \text{tanh}(h W_M + b_M) \\ M_{0:-1} \end{bmatrix}
$$

Where $h = T^{\text{out}}_{0}$, eg, the encoded "core" token.

Policy and value both come from $h$.

The observations are separated into pieces:
 - Core observations (non-factored, single vector)
 - Factored observations
 - Memory

Note that the input sequence can be variable length.

## Related Work

## Application to BabyAI

![[wmg_factored_babyai.png]]

WMG-factored gets good results in terms of sample efficiency for GoToRedBall, GoToLocal, PickupLocal , need about 16,000 steps.

The native observation space of BabyAI can be represented by a set of factors describing the types, colors and relative x/y co-ordinates of all objects in its field of view, eg (green, key, 3, 1), (green, ball, 2, 2), (red, key, 0, 3). A factored observation is more compact and is embedded into a factor vector which serves as input into the transformer.

Note that the instruction is also factored, but it goes into "core". There's a lot of "design" that goes into how the bits of the observation are funneled into different parts of the network.

There is a synergy between Transformer-based RL architectures and "factored observations", demonstrating that by virtue of self-attention, WMG learn policies in a sample efficient manner.