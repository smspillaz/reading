---
title: Revisiting Experience Replay in Non-Stationary Environments
---
# Revisiting Experience Replay in Nonstationary Environments

[[revisiting_experience_replay_non_stationary_environments.pdf]]

Shows that some nonstationary environments can be harmed by experience replay that is unformly sampled

Proposes a recency-biased correction scheme that improves the robustness of learning from non-stationary environments

Prior work with nonuniform sampling:
 - Actively sample based on uncertainty
 - Sample based on informativeness and diversity
 - Variance reduction


## Prioritise replay

### Prioritised Exeprience Replay (PER)

 - Use experience TD error to approximate suprise and re-weighs experience for sampling
 - $P(i) = \frac{\delta(i)^{a}}{\sum_{k} \delta(k)^a}$, $a > 0$
 - Where $a = 0$ - uniform sampling. Wjhen $a > 0$, bias towards samples with higher TD-error ($\delta$).


### Principles

![[cer_recently_biased_sampling_experience_replay.png]]

 - Extend Combined Expereince Replay, by injecting a recency bias into the sampling weights.
 - Discounts priorities of state transition tuples based on recency adjusted adjusted by how frequently they get added to the replay buffer.
 - Reasons:
	 - Prioritising the more recent experience in training helps the agent to learn from on-policy data
	 - Adapting the priorities of experiences based on their underlying states ratehr than individual transitions helps maintain a good sample coverage.
	 - Uniform sampling favours transitions that are more abundant in the buffer.
 - Probability:
	 - $P(s, a) \propto \exp(-r \cdot N(s, a))$, where $N(s, a)$ is the "number of times the state has been sampled" and $r$ si the scaling factors


### Implementation details

![[sarsa_nn_with_recency_adjusted_per.png]]

 - We want to discount sampling weights of a state rather than an individual experience
 - But we only have experiences, adjusting weights of experiences doesn't account for frequency of experience tuples generated by the same states.
 - Approximate by memorizing a cosine similarity matrix among all experiences in the buffer
 - On each sampled batch
	 - (1) Calculate and update similarity scores among states in the batch
	 - (2) Update $N$ by adding the sum of similarities per experience as a result


### Experiemnts

 - Blocking maze
 - Non-stationarity introduced by environment changes in midle of hte run
 - Once the agent solves it, close the path and open another one.
 - Agent must learn a new path
 - Old memories don't help you so CER fails