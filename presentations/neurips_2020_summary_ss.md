# Papers

Hierarchical:
 - HIDe
 - Accelerated reinfrocement learning with learned skill priors

Exploration / curriculm:
 - IMAGINE
 - Provably Efficient Exploration
 - Self-paced DRL
 - Sample efficient RL of undercomplete POMDPs
 - Planning from Pixels using Inverse Dynamics Models
 - Novelty Search in the Representational Space for Sample Efficient Exploration

Imitation Learning / Offline Learning:
 - Imitation learning w/o policy optimiztion
 - Critic regularized regression

## HIDe

[[hide_functionally_decomposed_hierarchy.pdf]]

[[deep_rl_workshop]]

tl;dr: Hindsight-based hierarchical policy, explicit separation of concerns

 - Status quo:
	 - Hierarchical policy with fixed or varying time commitment (HAC / HIRO / HiPPO)
		 - "Manager" sets a "goal state" for the low-level policy
		 - "Worker" does actions to try and reach the "goal"
	 - Hindsight Experience Replay: Relabel past trajectories done by the low-level policy with the "goal" that the trajectory would have achieved
 - Problem: Goal-setting and goal-achieving are fundamentally different problems, no need for both of them to be dependent on the same state space
	 - Observation: "Manager" and "Worker" should have different observation spaces
	 - Worker should only learn to reach the goal, however that goal is set
	 - Manager should only learn to set goals

Inputs:
 - Manager: Sees the overall goal of the task, map of the state space
 - Worker: Sees only its immediate environment and the goals passed by the manager


Manager:
 - MVProp:
	 - Locate the goal state in the gridworld
	 - Recursively label neighbouring states with 1 if rewarding, else gamma times highest neighbour
 - Action:
	 - Look at agent's position
	 - Attention map defined by variable-sized Gaussian
	 - Pick some nearby state and label it as the goal

Worker:
 - Go to the goal state

![[hide_arch.png]]

Nice things:
 - Allows for transfer / adaptation to unseen environment


## Self-Paced DRL

[[self_paced_deep_reinforcement_learning.pdf]]

[[rl_31#Self-paced Deep Reinforcement Learning]]

Context: Curriculums are nice. One problem is figuring out a principled way to generate the curriculum, which includes the tasks that you have in the curriculum and also when to move from one task to the next.

This work: Take a distributional approach.
 - Properly trained agent should be able to perform tasks drawn from p(x)
 - Agent in its infancy can perform tasks from q(x), where q(x) is a subset of p(x)
 - Variational optimization problem: find q(x) that maximizes reward but is similar to p(x)

Introduce "CMDP" framework:

$$
J(\mu, \pi) = E_{\mu(c),p_{\pi}(\tau|c)} [\sum_t \gamma^t r_c(s_t, a_t)]
$$

The utility, $J$ is a function of $\mu$ (some distribution over contexts, $c$ and $\pi$, the policy).

The utility is the expected return - discounted rewards that come from trajectories of states and actions generated by some policy in a given context.

$$
p_{\pi}(\tau|c) = p_{0, c}(s_0) \prod p_c (s_{t + 1}|s_t, a_t)\pi(a_t|s_t, c)
$$

The probability that you get a given trajectory for a context is the probability of starting in a given $s_0$, times the product of transitions: the probability of seeing a state transition for a context distribution, times the probability that the policy generates action which generates that state transition.

Our "true" distribution over task contexts is $\mu(c)$. We can make another distribution $p_{v}(c)$ which is an "training" distribution (so instead of drawing contexts from $\mu(c)$ we draw them from $p_v(c)$ instead):

$$
J(v, \pi) = E_{p_v(c),p_{\pi}(\tau|c)} [\sum_t \gamma^t r_c(s_t, a_t)]
$$

then we can have the following optimization objective

$$
\max J_{v, \pi}(v, \pi) - \alpha D_{\text{KL}}(p_v(c)||\mu(c))
$$

The optimization then becomes a trade-off between making the task distribution more like the real one vs adjusting the task distribution to maximize reward. But as the policy gets better, we can shift the task distribution back towards the real one without any penalty.

Different ways to optimize:

 - Adjust $\alpha$ during training
 - Expectation maximization


Algorithm in practice:

 - Sample contexts from p_v(c)
 - Rollout trajectories in each context
 - Improve policy
 - Optimize J with respect to p_v parameters given pi

![[self_paced_experiment_envs.png]]

![[self_paced_experiment_ant_ball_reward.png]]

## Critic Regularized Regression

[[offline_rl_workshop#Critic Regularized Regression]]

[[critic_regularized_regression.pdf]]

Offline RL:
 - Collect some dataset and learn a policy from that
 - Problems:
	 - extrapolation past observed data
	 - learn actions that are not good, irrelevant to the task
	 - Q-estimates are overly-confident


Define dataset $B$ as the one that contains all the collected trajectories.

Learn the Q-function by minimizing:

 - $E_B [D(Q_{\theta}(s_t, a_t), (r_t + \gamma E_{a \sim \pi(s_{t + 1})} Q_{\theta'}(s_{t + 1}, a)))]$

Parametric policy might propose actions not in $B$, where $Q$ may give a wrong estimate.

Avoid evaluating $Q$ for $(s, a) \not \in B$

Train $\pi$ by discouraging actions outside the trinaing dist. Hard to do with policy gradients. Change objective to match mapping contained in training data.

$$
\arg \max_{\pi} E_{(s, a) \sim B}[f(Q_{\theta}, \pi, s, a) \log \pi(a|s)]
$$

Here, $f$ is some non-negative function increasing in $Q_{\theta}$. Authors suggest that $f$ should be some indicator function of the advantage (actual reward - estimated reward):

$$
1[\hat A_{\theta}(s, a) > 0]
$$

![[critic_regularized_regression.png]]

Algorithm:
 - Sample some $(s_t, a_t, r_t, s_{t + 1})$ from $B$
 - Update actor by taking $\log \pi_{\phi}(a_t|s_t) f(Q_{\theta}, \pi_{\theta}, s_t, a_i)$ as the loss
	 - negative-log-likelihood of the policy times "advantage" over estimated Q function, ignoring actions where there is no change
 - Update critic by taking $D[Q_{\theta}(s_t, a_t), (r_t + \gamma E_{a \sim \pi(s_{t + 1})} Q_{\theta'}(s_{t + 1}, a))))]$
	 - $D$ can be L2, but in the paper they use a variational measure
 - Slow update of target actor/critic nets by copying parameters every $N$ steps (stability).

## Honourable Mentions

### Novelty Search in representational space

[[novelty_search_representational_space.pdf]]

[[novelty_search_representational_space_sample_efficient]]

tl;dr: Take an information-theoretic approach to representation encoding, give "novelty bonus" to unseen states only if they contain important unseen information.

 - Loss function: Encoding rate minus entropy gain

In practice:

 - Learn some encoder that helps you to predict Q
 - Keep transition buffer
 - Train dynamics model and Q function with it
	 - Also trains the encoder
 - Intrinsic reward is the distance to the k-nearest neighbours in the tranisition buffer in the representational space.


Nice experiments showing that eg, maze-navigation "holding key" sits on a different manifold in the representational space.

### IMAGINE

tl;dr: Generate language statements to explore the environment compositionally. Social partner tells you what you did in the environment.

![[imagine.png]]

Lots of work in the paper which summarizes how humans learn through play.

![[imagine_arch.png]]

Architecture:
 - Goal generator: samples from known and imagined goals
 - Agent interacts with policy conditioned on goal
 - Store trajectories in replay buffer
 - Use the social partner descriptions to add goals.

### Planning from Pixels using Inverse Dynamics Models

tl;dr: MBRL, bayesian perspective. Maximize the relative gain in probability of reaching the goal given some actions. Train a world model then use random shooting.

 - Action prior disentangles the probability of reaching the goal from the probability of taking the action sequence.

General idea: If we have a model, then we can plan the action.

Permit factoring across the actions in the action sequence. Allows us to do heuristic search.

$$
p(s_k = g|s_1, a_1, ..., a_{k - 1}) \propto \prod^{k - 1} \frac{p(s_k = g|s_1, a_{< i},a_i)}{p(s_k = g|s_1, a_{< i})}
$$

The ratio is the "relative gain in probability of reaching a goal state conditioned on action $a_i$ versus marginal probability if we did not condition on that action from the current trajectory". Basically - does
action $a_i$ help us?

Bayes rule says that we can find the marginal probability of the
action given the goal state, current state and previous action:

$$
\frac{p(a_i|s_1,s_k = g,a_{\le i - 1})}{p(a_i|s_1, a_{\le i - 1})} = \frac{p(s_k = g|s_1, a_{< i},a_i)}{p(s_k = g|s_1, a_{\le i - 1})}
$$

$p(a_1, ..., a_k|s_1, a_{\le i})$ is the "inverse dynamics model".
and $p(a_1, ..., a_k|s_1)$ is the "action prior".

The optimal plan is then given by 

$a_1^*, ..., a_{k - 1}^* = \arg \max_{a_1, ..., a_{k - 1}} \gamma^k \frac{p(a_1, ..., a_k|s_1, a_{\le i})}{p(a_1, ..., a_k|s_1)}$

Eg, the discounted inverse dynamics over the action prior.

How to learn the inverse dynamics?

Maximizes the negative log likelihood of seeing the actions given the states in the model and maximize the likelihoood of the action prior given the first state (9).

To find the optimal actions: random shooting.

Training Algorithm:
 - Sample some trajectories with a policy and goals
 - Add those tuples to your dataset
 - Update with gradient descent by (9).



# Nicola

## Emergent Compelxity and zero-shot learning via Unsupervised Environment Design

basic idea: parameterized environment and learn policy that generates distribution over environment's parameter. Should be solveable.

 - Domain randomization
 - Adversarial Training


Underspecified POMDPs

 - Free parameters needed to fully specify the POMDP
 - Protagonist Agent
 - Antagonist Agent
 - Total regret: Antagonist - Protagonist
 - Adversary Environment policy
	 - Maximize regret

Kind of like self-play

Algorithm:
 - Collect trajectories from protagonist and antagonist
 - Train policies
 - Train adversary policy with reward as the total regret of the protagonist
	 - Choose parameters that exploit the weaknesses of P


When the difficulty of the environment is increased to become more difficult  - other approaches drop in performance.

Adversary can choose parameters during episode too.

The PAIRED adversary must maintain feasible parameters.

Thoughts:
 - Unsupervised environment desgin can lead to automated curriculum design
 - Decision theory - game theory
 - Partial parameterization of the environment is required. Hard if the environment engine is not accessible.
 - Regret is a single number - not a discounted reward.
 - Unclear how the update is done for the adversary 


Question: So you have two agents, agent #2 gets to design the environment

Thought: more than two agents - every agent gets a "turn" to pick the environment to fool the other agents.

 - But note: will this improve sample efficiency?

## Sample-Efficient DRL

Hyper-parameter optimization and NAS
 - Turning HPs is expensive in computational cost and # interactions with environment


Proposal: Evolutionary approach to joint population training with different HPs, shared memory replay

Related work:
 - Random search, black box optimization
 - Population based training (genetic optimization), doesn't share experience with population

Main challenges:
 - Can you share information with each other?


SE-AutoRL:

 - Evaluate fitness of each agent in the environment
	 - Select k agents and keep the most fit one
	 - Applly mutations to selected individuals
	 - Train each individual usign experience from a replay memory and RL algorithm
		 - But this is off-policy?
			 - But not quite off-policy, since you only pick the best policy each time and then tweak the hyperparameters
			 - Changing things like activations etc would be problematic
		 - I guess with things like different learning rates etc


Ideas:
 - Sharing experience from different actors ujsing off-policy - up to linear gains in sample-efficiency, driven by numberof unique transitions
 - Shift from single-learner with N actors to N learners with N actors.


- Nice thing about this: you don't need to pick hyperparameters

# Arturs
 - Unsupervised object recognition
 

## Space-Time Correspondence as a Constrastive Random Walk

![[space_time_correspondence_random_walk.png]]

Model sequences of images as palindromes.

When you have this, you can get self-supervision.

Model trajectory of each superpixel as time in a graph
 - Not every superpixel travels in a trajectory but it fans out
 - Performs well on downstream tasks based on label propagation.
 - Works on natural images


Label super-pixels in the first frame and see where they go in the graph.

Video as a Graph:
 - Between frames, compute distance-based attention for every superpixel $i$ in $t$ - see which superpixels $j$ in $t + 1$ are close. Get probability distribution across rows.
 - Compute between every frame
 - Gives you a trajectory matrix
 - Think of it like a soft-permutation matrix - which superpixel travels to which place.
 - No labels - we need to unroll it back - concatenate the same thing in reverse order. Hope that the products get you back to where you started.

Look at the final matrix that you get at the end of multiplying all the attentions and see if it has ones on a diagonal and take the log.

- Segment the image into jittered superpixels. Not exactly aligned but jittered. This regularized a little and there are some gaps (unfold)
	- Take an image 256x256, create 7x7 49x49 patches and stack them
	- Embed the patches separately through Resnet18
- for every frame between 0 and t, compute attention. Attention from t to t + 1
- Concat the trajectories in reverse order
- Drop some edges (nearby superpixels or patches will contain shared information). There should not be a single edge between every consecutive superpixel.
- Compute walks by multiplying out matrix
- Compute the loss
	- Hope that the walk gives you back at the identity
		- Eg, start at identity, go to end, go back to beginning - should end up at ident
- How do you ensure that your attention is not always just identity


Label propagation:
 - Assign labels and see how the regions propagate all the way to the end
 - Segmentation propagation
 - Bounding box propagatioon
 - Pose propagation


What is the output?
 - For every image it learns a representation where object-like regions will be encoded in the same way

What if the superpixel is occluded?
 - There is some overlap in receptive fields - some of the information will leak through the occluded.


## Better Set Representations for Relational Reasoning

 - DSPN without set-based training labels
 - Encodes images into sets
 - Set-encoder just like in DSPN

Most naive ways of extracting a set have a "responsibility problem"
 - Each superpixel corresponds to an object, objects are partitioned to superpixels. No single-feature for an object. Discontinuity


Relational Reasoning System SRN:
 - Inner loop optimizer, image encoder takes image X and outputs sets at timesteps
	 - Initially just superpixels
	 - Competition
	 - Inner loop optimization where the loss is the MSE between the aggregation of the set compared to the vector $z$ - so basically you do MPC internally.
	 - FSPool: Preserve set-like information in a single-vector. Does sorting/weighing a bunch of magic etc - takes a list of vectors and compresses into a representation such that this representation is easy to compress back into the set.


Main take-away: Take DSPN, change on thing and you have images, sets and no set labels.

 - Reconstruction of the image
 - Performs quite well on simple 2D images.

Unclear how this is better than slot-attention

## Learning Physical Graph Representations from Visual Scenes (PSGNet)

[Link](https://neuroailab.github.io/physical-scene-graphs)

 - Proposes a graph-based image representation and a model to extract it
 - Think about how to model an image from a graph - model 3D space as an object graph wth latent data with matching real-world attributes (color, depth, normals)
 - Apply this to movies, still images
 - Inverse rendering, etc

Approach: Pooling pixels - which pixels belong to which node?

They claim that they are self-supervised but they have datasets, but you get the depth index and normal index of the 3D objects. In some applications you have that data but they're trying to get rid of this dependence on depth and normal information.

Model:
 - Physical Scene graph
	 - Express an image into a graph
	 - Take the primitives and group them according to some principles
	 - At every layer they use different grouping principles
		 - Different inductive biases at each layer for the pooling


Building the graphs:
 - Extract visual features
	 - ConvRNN
	 - U-Net style summation - add the representation that you learn at each level back to the representation on the bottom levels
	 - The higher-level layers are a smaller spatial dimension
 - Graph construciton
	 - Image is a graph to start with
	 - Layers of hierarchy and pool with GCN layers in between.
	 - Add the edges, do label propagation, then merge things with similar layers based on principles
 - Perceptual grouping using principles from human psychology
	 - Similarity
	 - Co-occurrence
	 - Common fate
		 - If it moves together, probably the same thing
	 - Self-supervision - do they move together?

![[physical_graph_representation_pooling_steps.png]]

![[physical_graph_representation_encoding.png]]

Decoding:
 - Quadratic shape rendering / texture rendering. Output is a 3D rendered textureless model of the input.
 - The hope is that you match the ground truth better - loss calculated on:
	 - RGB
	 - Depth
	 - Shapes (self-supervised)
	 - Camera projection (focal length unknown)


They apparently beat IODINE and and Monet